[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Projects",
    "section": "",
    "text": "A Replication of Karlan and List (2007)\n\n\n\n\n\n\nErin Yabsley\n\n\nMay 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nErin Yabsley\n\n\nMay 6, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/project1/hw1_questions.html",
    "href": "blog/project1/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe paper investigates how matching grants influence charitable donations. Karlan and List (2007) randomized 50,083 donors into three groups: control, matching, and challenge. The matching group was subdivided further by match ratios (1:1, 2:1, 3:1) and match thresholds ($25k, $50k, $100k, unspecified). Letters also varied in suggested ask amount. The study tested how these variations affected the likelihood someone donates (response rate) as well as the amount they give. The experiment varied the maximum match amount offered ($25K, $50K, $100K, or unspecified) and the suggested donation amount (based on past giving).\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/project1/hw1_questions.html#introduction",
    "href": "blog/project1/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe paper investigates how matching grants influence charitable donations. Karlan and List (2007) randomized 50,083 donors into three groups: control, matching, and challenge. The matching group was subdivided further by match ratios (1:1, 2:1, 3:1) and match thresholds ($25k, $50k, $100k, unspecified). Letters also varied in suggested ask amount. The study tested how these variations affected the likelihood someone donates (response rate) as well as the amount they give. The experiment varied the maximum match amount offered ($25K, $50K, $100K, or unspecified) and the suggested donation amount (based on past giving).\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/project1/hw1_questions.html#data",
    "href": "blog/project1/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import t\nimport statsmodels.formula.api as smf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\n\ndata = pd.read_stata('karlan_list_2007.dta')\nprint(data.head(5))\n\nprint(\"Data shape:\", data.shape)\n\n   treatment  control    ratio  ratio2  ratio3      size  size25  size50  \\\n0          0        1  Control       0       0   Control       0       0   \n1          0        1  Control       0       0   Control       0       0   \n2          1        0        1       0       0  $100,000       0       0   \n3          1        0        1       0       0  Unstated       0       0   \n4          1        0        1       0       0   $50,000       0       1   \n\n   size100  sizeno  ... redcty  bluecty    pwhite    pblack  page18_39  \\\n0        0       0  ...    0.0      1.0  0.446493  0.527769   0.317591   \n1        0       0  ...    1.0      0.0       NaN       NaN        NaN   \n2        1       0  ...    0.0      1.0  0.935706  0.011948   0.276128   \n3        0       1  ...    1.0      0.0  0.888331  0.010760   0.279412   \n4        0       0  ...    0.0      1.0  0.759014  0.127421   0.442389   \n\n   ave_hh_sz  median_hhincome    powner  psch_atlstba  pop_propurban  \n0       2.10          28517.0  0.499807      0.324528            1.0  \n1        NaN              NaN       NaN           NaN            NaN  \n2       2.48          51175.0  0.721941      0.192668            1.0  \n3       2.65          79269.0  0.920431      0.412142            1.0  \n4       1.85          40908.0  0.416072      0.439965            1.0  \n\n[5 rows x 51 columns]\nData shape: (50083, 51)\n\n\nThe dataset has 50,083 observations and 51 columns. The fields included in the dataset assign individuals to either control or treatment, the specifications for the treatment group, donation history of an individual, as well as some socioeconomic & political & demographic data about the individual. The variables and their descriptions are outlined below:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\nI define a function to print results from regressions in a readable format:\n\ndef print_clean_regression_results(model, variable_names=None):\n    results = model.params.to_frame('Coefficient')\n    results['Std. Error'] = model.bse\n    results['t'] = model.tvalues\n    results['p-value'] = model.pvalues\n    results = results.round(4)\n\n    if variable_names:\n        results.index = [variable_names.get(var, var) for var in results.index]\n\n    print(results)\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\nI create a function t_test_manual that breaks down the steps to compute a t-statistic:\n\ndef t_test_manual(x_treat, x_control):\n    mean_treat = np.mean(x_treat)\n    mean_control = np.mean(x_control)\n    \n    var_treat = np.var(x_treat, ddof=1)\n    var_control = np.var(x_control, ddof=1)\n    \n    n_treat = len(x_treat)\n    n_control = len(x_control)\n\n    t_stat = (mean_treat - mean_control) / np.sqrt((var_treat / n_treat) + (var_control / n_control))\n\n    df_num = (var_treat / n_treat + var_control / n_control) ** 2\n    df_denom = ((var_treat / n_treat) ** 2 / (n_treat - 1)) + ((var_control / n_control) ** 2 / (n_control - 1))\n    df = df_num / df_denom\n\n    p_value = 2 * (1 - t.cdf(np.abs(t_stat), df))\n    return t_stat, p_value\n\nI define the variables that I want to test: the number of months since last donation (mrm2), the highest previous contribution (hpa), the number of prior donations (freq), and the number of years since initial donation (years). I test these variables at the 95% confidence level and run a linear regression on each of the variables and look at the estimated coefficient on the treatment variable:\n\nvariables = ['mrm2', 'hpa', 'freq', 'years']\n\nfor var in variables:\n    treat_vals = data[data['treatment'] == 1][var].dropna()\n    control_vals = data[data['treatment'] == 0][var].dropna()\n\n    t_stat, p_val = t_test_manual(treat_vals, control_vals)\n\n    print(f\"T-test for {var}: t-statistic = {t_stat:.4f}, p-value = {p_val:.4f}\\n\")\n    model = smf.ols(f\"{var} ~ treatment\", data=data).fit()\n    print_clean_regression_results(model, variable_names={'Intercept': 'Intercept', 'treatment': 'Treatment'})\n    print()\n\nT-test for mrm2: t-statistic = 0.1195, p-value = 0.9049\n\n           Coefficient  Std. Error         t  p-value\nIntercept      12.9981      0.0935  138.9789   0.0000\nTreatment       0.0137      0.1145    0.1195   0.9049\n\nT-test for hpa: t-statistic = 0.9704, p-value = 0.3318\n\n           Coefficient  Std. Error         t  p-value\nIntercept      58.9602      0.5510  107.0054   0.0000\nTreatment       0.6371      0.6748    0.9441   0.3451\n\nT-test for freq: t-statistic = -0.1108, p-value = 0.9117\n\n           Coefficient  Std. Error        t  p-value\nIntercept       8.0473      0.0882  91.2313   0.0000\nTreatment      -0.0120      0.1080  -0.1109   0.9117\n\nT-test for years: t-statistic = -1.0909, p-value = 0.2753\n\n           Coefficient  Std. Error         t  p-value\nIntercept       6.1359      0.0426  144.0227     0.00\nTreatment      -0.0575      0.0522   -1.1030     0.27\n\n\n\nAt the 95% confidence level, none of the baseline variables differ significantly between the treatment and control groups as all of the p-values associated with the variables are larger than 0.05. This supports random assignment - it shows that baseline characteristics are balanced between the treatment and control groups. As a result, we can conclude that any observed differences in charitable giving outcomes are due to the treatment and not pre-existing group differences."
  },
  {
    "objectID": "blog/project1/hw1_questions.html#experimental-results",
    "href": "blog/project1/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\nI create a barplot that shows the proportion of people who donated in the control group and the treatment group:\n\ndonation_rates = data.groupby('treatment')['gave'].mean()\n\nplt.figure(figsize=(8, 6))\nsns.barplot(\n    x=donation_rates.index, \n    y=donation_rates.values, \n    palette=['skyblue', 'salmon']\n)\nplt.xticks([0, 1], ['Control', 'Treatment'])\nplt.ylabel('Proportion Donated')\nplt.title('Donation Rates: Control vs Treatment')\nplt.ylim(0, 0.03)\nplt.show()\n\n/var/folders/j9/vvtdz6r56zg0sw78kg9kfsk00000gn/T/ipykernel_57841/1576349855.py:4: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(\n\n\n\n\n\n\n\n\n\nNext, I run a t-test between the treatment and control groups on the binary outcome of whether any charitable donation was made:\n\ntreat_group = data[data['treatment'] == 1]['gave']\ncontrol_group = data[data['treatment'] == 0]['gave']\n\nt_stat, p_val = t_test_manual(treat_group, control_group)\nprint(f\"T-test on gave: t = {t_stat:.4f}, p-value = {p_val:.4f}\")\n\nT-test on gave: t = 3.2095, p-value = 0.0013\n\n\nI then run a linear regression on whether any charitable donation was made with treatment as the explanatory variable:\n\nlinear_model = smf.ols('gave ~ treatment', data=data).fit()\nprint_clean_regression_results(linear_model, variable_names={'Intercept': 'Intercept', 'treatment': 'Treatment'})\n\ntreatment_summary = linear_model.summary2().tables[1].loc['treatment']\n\nprint(f\"\\nLinear Regression on 'gave ~ treatment': \\nCoefficient on treatment: {treatment_summary['Coef.']:.4f}, \\nt: {treatment_summary['t']:.4f}, p-value: {treatment_summary['P&gt;|t|']:.4f}\")\n\n           Coefficient  Std. Error        t  p-value\nIntercept       0.0179      0.0011  16.2246   0.0000\nTreatment       0.0042      0.0013   3.1014   0.0019\n\nLinear Regression on 'gave ~ treatment': \nCoefficient on treatment: 0.0042, \nt: 3.1014, p-value: 0.0019\n\n\nPeople in the treatment group were 0.42% more likely to donate than those in the control group, and this effect is statistically significant at a 95% confidence level as the p-value for the treatment variable is 0.0019 (0.0019 &lt; 0.05). This supports the idea that matching gift offers nudge more people to give, even if the absolute effect seems small.\nWhat we learn about human behavior: Social & Psychological Motivation\nEven a small match taps into social proof and reciprocity, which are powerful motivators for charitable giving. The presence of a matching gift offer can create a sense of urgency and social pressure to contribute, leading to increased donations. This aligns with the concept of social norms, where individuals are influenced by the behavior of others in their social group. The matching gift offer serves as a signal that others are also contributing, which can motivate individuals to join in and support the cause.\nDonation behavior are not purely altruistic - if it was, we might expect people to give regardless of match framing. Instead, people are motivated by cues about effectiveness and urgency, suggesting many give in part for “warm glow” satisfaction - the emotional reward of feeling generous and impactful.\nI now run a probit regression where the outcome variable is whether any charitable donation was made and the explanatory variable is assignment to treatment or control:\n\nprobit_model = smf.probit('gave ~ treatment', data=data).fit()\nprint_clean_regression_results(probit_model, variable_names={'Intercept': 'Intercept', 'treatment': 'Treatment'})\n\nmarginal_effects = probit_model.get_margeff()\nprint(marginal_effects.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n           Coefficient  Std. Error        t  p-value\nIntercept      -2.1001      0.0233 -90.0728   0.0000\nTreatment       0.0868      0.0279   3.1129   0.0019\n       Probit Marginal Effects       \n=====================================\nDep. Variable:                   gave\nMethod:                          dydx\nAt:                           overall\n==============================================================================\n                dy/dx    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\ntreatment      0.0043      0.001      3.104      0.002       0.002       0.007\n==============================================================================\n\n\nThe positive coefficient on treatment means that receiving a matching offer increases the probability of donating. The treatment effect is positive and is statistically significant at a 1% significance level: the p-value associated with the treatment variable is 0.002 as seen in the probit regression. This suggests humans are sensitive to social validation when making charitable decisions.\nThe results from the probit regression confirm the results reported in Table 3 of the paper. From our probit regression, I find that the marginal effect of the treatment is 0.0043 with a standard error of 0.001, and column 1 of Table 3 reports a marginal effect of 0.004 (rounded) with standard error 0.001\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\nI conduct a series of t-tests to test whether the size of the match ratio has an effect on whether people donate or not:\n\nratios = [1, 2, 3]\npairwise_results = {}\n\nfor i in range(len(ratios)):\n    for j in range(i + 1, len(ratios)):\n        r1, r2 = ratios[i], ratios[j]\n        group1 = data[data['ratio'] == r1]['gave']\n        group2 = data[data['ratio'] == r2]['gave']\n\n        t_stat, p_val = t_test_manual(group1, group2)\n        pairwise_results[f'{r1}:{r2}'] = {'t_stat': t_stat, 'p_value': p_val}\n\nprint(\"Pairwise t-test Results (Donation Rates by Match Ratio):\")\nfor comparison, result in pairwise_results.items():\n    print(f\"{comparison} \\t t-stat: {result['t_stat']:.4f}, p-value: {result['p_value']:.4f}\")\n\nPairwise t-test Results (Donation Rates by Match Ratio):\n1:2      t-stat: -0.9650, p-value: 0.3345\n1:3      t-stat: -1.0150, p-value: 0.3101\n2:3      t-stat: -0.0501, p-value: 0.9600\n\n\nNone of the differences across match ratios are significant at a 5% significance level. All of the p-values are larger than 0.05 (0.3345, 0.3101, 0.9600).\nThis demonstrates that higher match ratios, such as 3:1 compared to 1:1, do not lead to a statistically significantly greater increase in donor participation. A basic 1:1 match performs almost as well, making it a more cost-effective option. This indicates that nonprofits can generate comparable donor response using lower match ratios, which may help maximize the impact of their fundraising resources.\nFrom the paper, the author makes the comment: “the figures suggest that neither the match threshold nor the example amount had a meaningful influence on behavior.” The results of the t-test reinforce this comment as the different match ratios do not have a significant effect on the donation.\nI approach the same problem using a regression approach. I create the variable ratio1 then regress gave on ratio1, ratio2, and ratio3:\n\ndata['ratio1'] = (data['ratio'] == 1).astype(int)\nregression_model = smf.ols('gave ~ ratio1 + ratio2 + ratio3', data=data).fit()\nprint_clean_regression_results(regression_model, variable_names={\n    'Intercept': 'Intercept',\n    'ratio1': 'Match 1:1',\n    'ratio2': 'Match 2:1',\n    'ratio3': 'Match 3:1'\n})\n\n           Coefficient  Std. Error        t  p-value\nIntercept       0.0179      0.0011  16.2245   0.0000\nMatch 1:1       0.0029      0.0017   1.6615   0.0966\nMatch 2:1       0.0048      0.0017   2.7445   0.0061\nMatch 3:1       0.0049      0.0017   2.8016   0.0051\n\n\nI interpret the coefficients and their statistical precision: the intercept (1.79%) is the baseline donation rate for the control group. The ratio1 (1:1) adds +0.29% to that (total ≈ 2.68%), but the result is significant at a 90% confidence level (p-value ≈ 0.096). ratio2 (2:1) and ratio3 (3:1) both raise donation rates by 0.48 and 0.49% respectively to 2.66 and 2.68% and are statistically significant at the 1% level (with p-values of 0.0061 and 0.0051 respectively).\nI calculate the response rate difference between the 1:1 and 2:1 match ratios and the 2:1 and 3:1 ratios. I do this directly from the data:\n\nresponse_rates = data.groupby('ratio', observed=False)['gave'].mean()\n\ncoef_ratio2 = regression_model.params['ratio2']\ncoef_ratio3 = regression_model.params['ratio3']\nintercept = regression_model.params['Intercept']\n\ndiff_data_2_1 = response_rates[2] - response_rates[1]\ndiff_data_3_2 = response_rates[3] - response_rates[2]\n\nprint(\"Response Rates from Raw Data:\")\nprint(f\"1:1 match = {response_rates[1]:.4f}\")\nprint(f\"2:1 match = {response_rates[2]:.4f}\")\nprint(f\"3:1 match = {response_rates[3]:.4f}\")\nprint(f\"Difference (2:1 - 1:1): {diff_data_2_1:.4f}\")\nprint(f\"Difference (3:1 - 2:1): {diff_data_3_2:.4f}\")\n\nResponse Rates from Raw Data:\n1:1 match = 0.0207\n2:1 match = 0.0226\n3:1 match = 0.0227\nDifference (2:1 - 1:1): 0.0019\nDifference (3:1 - 2:1): 0.0001\n\n\nThe difference from the raw data between the 1:1 match and the 2:1 match is 0.0019 and the difference from the raw data between the 1:1 match and the 3:1 match is 0.0001.\nI now calculate the response rate difference between the 1:1 and 2:1 match ratios and the 2:1 and 3:1 ratios with the fitted coefficients from the regression:\n\npred_1to1 = intercept\npred_2to1 = intercept + coef_ratio2\npred_3to1 = intercept + coef_ratio3\n\ndiff_model_2_1 = pred_2to1 - pred_1to1\ndiff_model_3_2 = pred_3to1 - pred_2to1\n\nprint(\"\\nPredicted Donation Rates from Regression Model:\")\nprint(f\"1:1 match = {pred_1to1:.4f}\")\nprint(f\"2:1 match = {pred_2to1:.4f}\")\nprint(f\"3:1 match = {pred_3to1:.4f}\")\nprint(f\"Difference (2:1 - 1:1): {diff_model_2_1:.4f}\")\nprint(f\"Difference (3:1 - 2:1): {diff_model_3_2:.4f}\")\n\n\nPredicted Donation Rates from Regression Model:\n1:1 match = 0.0179\n2:1 match = 0.0226\n3:1 match = 0.0227\nDifference (2:1 - 1:1): 0.0048\nDifference (3:1 - 2:1): 0.0001\n\n\nThe difference from the regression model between the 1:1 match and the 2:1 match is 0.0048 and the difference from the regression model between the 1:1 match and the 3:1 match is 0.0001.\nThere is no strong evidence that offering larger match ratios (e.g., 3:1 instead of 1:1) is more effective in increasing the number of donors. A simple 1:1 match is nearly just as effective, making it a more cost-efficient fundraising strategy. This suggests that organizations can achieve similar results with lower match ratios, which can be beneficial for their fundraising efforts.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\nI run a bivariate linear regression of the donation amount on the treatment status:\n\nreg_all = smf.ols(\"amount ~ treatment\", data=data).fit()\nprint(\"Regression on all participants (includes non-donors):\")\nprint_clean_regression_results(reg_all, variable_names={'Intercept': 'Intercept', 'treatment': 'Treatment'})\n\nRegression on all participants (includes non-donors):\n           Coefficient  Std. Error        t  p-value\nIntercept       0.8133      0.0674  12.0630   0.0000\nTreatment       0.1536      0.0826   1.8605   0.0628\n\n\nPeople in the control group gave $0.81 on average (including non-donors). Being in the treatment group increases the average donation by $0.15, a 19% increase, but this is only significant at a 10% significance level (p ≈ 0.06). Most people didn’t donate, making the average low because the dataset includes a lot of zeros.\nNow I limit the data to just people who made a donation and repeat the previous analysis. This regression allows me to analyze how much respondents donate conditional on donating some positive amount:\n\ndata_donated = data[data['gave'] == 1]\nreg_conditional = smf.ols(\"amount ~ treatment\", data=data_donated).fit()\nprint(\"\\nRegression on donors only (conditional on giving):\")\nprint_clean_regression_results(reg_conditional, variable_names={'Intercept': 'Intercept', 'treatment': 'Treatment'})\n\n\nRegression on donors only (conditional on giving):\n           Coefficient  Std. Error        t  p-value\nIntercept      45.5403      2.4234  18.7921   0.0000\nTreatment      -1.6684      2.8724  -0.5808   0.5615\n\n\nThe average donation among donors that gave is $45.54 in the control group. This is the intercept of the regression and is statistically significant at a 1% significance level given its p-value is ~0.\nDonors who received the matching grant treatment gave $1.67 less than control donors. This difference is not statistically significant (p = 0.561).\nThis regression confirms that among people who already decided to donate, the amount they gave did not increase with a match incentive. This confirms the statement in the paper that reads “The match increases participation, but does not significantly affect the donation amount conditional on giving.” The treatment coefficient has a causal interpretation about the effect on donors since this regression is conditional on giving. Donors were randomly assigned to treatment (match) or control (no match) and random assignment ensures that treatment is independent of unobserved confounders. So the coefficient on treatment estimates the causal effect of being offered a match.\nI make two plot: one for the treatment group and one for the control. Each plot shows a histogram of the donation amounts only among people who donated:\n\nfig, axs = plt.subplots(1, 2, figsize=(8, 5), sharey=True)\n\ncontrol_donated = data_donated[data_donated['treatment'] == 0]['amount']\nmean_control = control_donated.mean()\nsns.histplot(control_donated, bins=30, ax=axs[0], color=\"skyblue\", kde=False)\naxs[0].axvline(mean_control, color='red', linestyle='--', label=f\"Mean = {mean_control:.2f}\")\naxs[0].set_title(\"Control Group (Donors Only)\")\naxs[0].set_xlabel(\"Donation Amount\")\naxs[0].legend()\n\ntreatment_donated = data_donated[data_donated['treatment'] == 1]['amount']\nmean_treatment = treatment_donated.mean()\nsns.histplot(treatment_donated, bins=30, ax=axs[1], color=\"salmon\", kde=False)\naxs[1].axvline(mean_treatment, color='red', linestyle='--', label=f\"Mean = {mean_treatment:.2f}\")\naxs[1].set_title(\"Treatment Group (Donors Only)\")\naxs[1].set_xlabel(\"Donation Amount\")\naxs[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nI observe a similar distribution of donation amounts in both control and treatment groups. The average donation is slightly higher in control, which aligns with the regression result. Most donations cluster below $100, with the majority of the data falling between $0 to $50, and we observe a long tail.\nThe treatment effect is effective in motivating more people to give, not by getting people to give more."
  },
  {
    "objectID": "blog/project1/hw1_questions.html#simulation-experiment",
    "href": "blog/project1/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\nFirst, we compute the actual difference in the data:\n\np_control_data = data[data['control'] == 1]['gave'].mean()\np_treatment_data = data[data['treatment'] == 1]['gave'].mean()\ndiff_data = p_treatment_data - p_control_data\n\nprint(f\"Difference: {diff_data:.4f}\")\n\nDifference: 0.0042\n\n\nFrom the observed data, I find that the observed difference between the treatment group and the control group is 0.0040.\n\nLaw of Large Numbers\nI now simulate 10,000 draws from the control distribution and 10,000 draws from the treatment distribution. I then calculate a vector of 10,000 differences. I present a histogram for the sampling distributions for the treatment group, the control group, and the difference between the two groups:\n\nnp.random.seed(42)\n\np_control = 0.018\np_treatment = 0.022\nn_simulations = 10000\nsample_size = 500\n\ncontrol_means = []\ntreatment_means = []\ndiffs = []\n\nfor i in range(n_simulations):\n    control_sample = np.random.binomial(1, p_control, sample_size)\n    treatment_sample = np.random.binomial(1, p_treatment, sample_size)\n\n    control_mean = control_sample.mean()\n    treatment_mean = treatment_sample.mean()\n    diff = treatment_mean - control_mean\n\n    control_means.append(control_mean)\n    treatment_means.append(treatment_mean)\n    diffs.append(diff)\n\ncontrol_means = np.array(control_means)\ntreatment_means = np.array(treatment_means)\ndiffs = np.array(diffs)\n\nfig, axes = plt.subplots(3, 1, figsize=(8, 12), sharex=True)\n\nsns.histplot(control_means, bins=50, kde=True, color='skyblue', ax=axes[0])\naxes[0].set_title('Sampling Distribution of Control Group Means')\naxes[0].set_ylabel('Frequency')\n\nsns.histplot(treatment_means, bins=50, kde=True, color='salmon', ax=axes[1])\naxes[1].set_title('Sampling Distribution of Treatment Group Means')\naxes[1].set_ylabel('Frequency')\n\nsns.histplot(diffs, bins=50, kde=True, color='slateblue', ax=axes[2])\naxes[2].set_title('Sampling Distribution of Differences (Treatment - Control)')\naxes[2].set_xlabel('Proportion Donated')\naxes[2].set_ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nI plot the cumulative average of the vector of differences:\n\nnp.random.seed(42)\n\nn_control = 10000\nn_treatment = 10000\np_control = 0.018\np_treatment = 0.022\n\ncontrol_draws = np.random.binomial(1, p_control, n_control)\ntreatment_draws = np.random.binomial(1, p_treatment, n_treatment)\n\ndifferences = treatment_draws - control_draws\n\ncumulative_avg = np.cumsum(differences) / np.arange(1, len(differences) + 1)\n\nplt.figure(figsize=(8, 5))\nplt.plot(cumulative_avg, label='Cumulative Average of Differences', color='slateblue')\nplt.axhline(y=p_treatment - p_control, color='red', linestyle='--', label=f\"True Difference = {p_treatment - p_control:.4f}\")\nplt.title(\"Cumulative Average of Differences in Donation Rates\")\nplt.xlabel(\"Number of Samples\")\nplt.ylabel(\"Cumulative Average (Treatment - Control)\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWhat the Chart Shows: X-axis: Number of samples (from 1 up to 10,000) Y-axis: Cumulative average difference in donation rates (Treatment - Control) Blue Line: The running (cumulative) average of the difference in donation rates Red Dashed Line: The “true” or overall difference in donation rates across all data = 0.0040\nThe Law of Large Numbers says that as you increase your sample size, the sample average will converge to the true population mean.\nWhen the sample size is small (i.e., the first few hundred donors), the cumulative average fluctuates wildly, up to nearly 0.04, due to the small sample size. As the sample grows, the cumulative average smooths out and converges around 0.004, the red line. This is Law of Large Numbers in action: with more data, the sample estimate gets closer to the true treatment effect.\n\n\nCentral Limit Theorem\nIn this section of the analysis, I generated sampling distributions of the difference in donation rates between treatment and control groups by repeatedly drawing samples of different sizes (50, 200, 500, and 1000) and calculating the average difference in donation rates for each draw. I plot the histograms of the distributions:\n\nnp.random.seed(42)\n\nsample_sizes = [50, 200, 500, 1000]\nn_iterations = 1000\n\nfig, axs = plt.subplots(2, 2, figsize=(8, 10))\naxs = axs.flatten()\n\nfor i, n in enumerate(sample_sizes):\n    diffs = []\n    for _ in range(n_iterations):\n        control_sample = np.random.binomial(1, p_control, n)\n        treatment_sample = np.random.binomial(1, p_treatment, n)\n        diff = treatment_sample.mean() - control_sample.mean()\n        diffs.append(diff)\n\n    sns.histplot(diffs, bins=30, ax=axs[i], color=\"skyblue\", kde=False)\n    axs[i].axvline(0, color='black', linestyle='--', label='Zero (No Effect)')\n    axs[i].axvline(np.mean(diffs), color='red', linestyle='--', label=f'Mean = {np.mean(diffs):.4f}')\n    axs[i].set_title(f\"Sample Size = {n}\")\n    axs[i].set_xlabel(\"Difference in Sample Means\")\n    axs[i].set_ylabel(\"Frequency\")\n    axs[i].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe 4 histograms above display the distributions of sample means for different sample sizes.\nAt the smallest sample size (n = 50), the distribution of differences is wide and relatively noisy, with zero appearing near the center of the distribution. This means that with small samples, even when there is a true difference between treatment and control groups, the observed difference can often appear close to zero purely by chance: the signal is weak and buried in variability.\nAs the sample size increases to 200, the distribution becomes more concentrated, and I begin to see that zero is slightly less central. When I reach sample sizes of 500 and especially 1000, the distributions are sharply peaked and much narrower. Importantly, in these larger samples, zero is clearly in the tail of the distribution, not the center. This means that the observed differences consistently reflect the true underlying effect (here, a 0.004 increase in donation probability due to treatment), and it becomes increasingly unlikely that such a difference would be due to random chance alone.\nThe Central Limit Theorem and the Law of Large Numbers are illustrated: as sample size increases, sampling distributions of means become approximately normal and converge around the true mean. It also reinforces the intuition behind statistical significance—larger samples make it easier to detect small but real effects because zero is no longer a plausible value under the observed distribution of outcomes."
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Website – MGTA 495 MKTG",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "blog/project2/hw2_questions.html",
    "href": "blog/project2/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\nWe read in the dataset for Blueprinty and view the first 5 rows of data to understand what our data contains:\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import t\nimport statsmodels.formula.api as smf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom scipy.optimize import minimize_scalar\nfrom scipy.optimize import minimize\nfrom numpy.linalg import inv\nfrom scipy.optimize import minimize, approx_fprime\nimport patsy\nimport warnings\nfrom scipy.special import gammaln\nfrom statsmodels.genmod.families import Poisson\nwarnings.filterwarnings('ignore')\n\nblueprinty_data = pd.read_csv('blueprinty.csv')\nblueprinty_data.head(5)\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\nWe build compute the mean of number of patents by customer status (i.e., customers and non-customers):\n\nblueprinty_data['iscustomer'] = blueprinty_data['iscustomer'].astype('category')\n\nmean_patents = blueprinty_data.groupby('iscustomer', observed=True)['patents'].mean()\nprint(\"Mean number of patents by customer status:\")\nprint(mean_patents.rename(index={0: 'Non-Customer', 1: 'Customer'}))\n\nMean number of patents by customer status:\niscustomer\nNon-Customer    3.473013\nCustomer        4.133056\nName: patents, dtype: float64\n\n\nThe average number of patents for non-customers is 3.473013 while the average number of patents for customers is 4.133056.\nNext, we build and compare histograms for non-customers and customers:\n\nfig, axes = plt.subplots(2, 1, figsize=(7, 8), sharex=True)\n\nsns.histplot(\n    blueprinty_data[blueprinty_data['iscustomer'] == 0],\n    x='patents',\n    binwidth=1,\n    kde=False,\n    stat='density',\n    ax=axes[0],\n    color='skyblue'\n)\naxes[0].set_title('Non-Customers')\naxes[0].set_ylabel('Density')\naxes[0].grid(True)\n\nsns.histplot(\n    blueprinty_data[blueprinty_data['iscustomer'] == 1],\n    x='patents',\n    binwidth=1,\n    kde=False,\n    stat='density',\n    ax=axes[1],\n    color='salmon'\n)\naxes[1].set_title('Customers')\naxes[1].set_xlabel('Number of Patents')\naxes[1].set_ylabel('Density')\naxes[1].grid(True)\n\nplt.suptitle('Patent Distribution: Customers vs Non-Customers', fontsize=16)\nplt.tight_layout(rect=[0, 0, 1, 0.95])\nplt.show()\n\n\n\n\n\n\n\n\nThe histograms reveal that both customers and non-customers exhibit right-skewed distributions in the number of patents, with most firms having relatively few patents and fewer firms achieving higher counts. Customers tend to cluster more heavily around the 0 to 4 patent range. Both customers and non-customers have long tails, indicating that while most firms have few patents, there are a small number of firms with a very high number of patents. The distribution for non-customers is slightly more concentrated around the lower patent counts compared to customers, suggesting that blueprinty could be effective in generating achieving patents.\nCustomers tend to have a higher number of patents than non-customers (as seen by a higher mean: 4.13 for customers and 3.47 for non-customers), with a noticeable increase in the 5 to 10 patent range, with customers having higher density in this range.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\nblueprinty_data['region'] = blueprinty_data['region'].astype('category')\n\nregion_counts = blueprinty_data.groupby('iscustomer', observed=True)['region'].value_counts(normalize=True).unstack()\nprint(\"Proportion of firms by region and customer status:\")\nprint(region_counts.rename(index={0: 'Non-Customer', 1: 'Customer'}))\nprint()\n\nregion_plot_data = blueprinty_data.copy()\nregion_plot_data['iscustomer'] = region_plot_data['iscustomer'].map({0: 'Non-Customer', 1: 'Customer'})\n\nplt.figure(figsize=(7, 6))\nsns.countplot(data=region_plot_data, x='region', hue='iscustomer')\nplt.title('Region Distribution by Customer Status')\nplt.xlabel('Region')\nplt.ylabel('Count')\nplt.legend(title='Customer Status')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\nage_summary = blueprinty_data.groupby('iscustomer', observed=True)['age'].describe()\nprint(\"Summary of firm age by customer status:\")\nprint(age_summary.rename(index={0: 'Non-Customer', 1: 'Customer'}))\nprint()\n\nplt.figure(figsize=(7, 6))\nsns.boxplot(data=blueprinty_data, x='iscustomer', y='age', palette='Set2')\nplt.xticks([0, 1], ['Non-Customer', 'Customer'])\nplt.title('Firm Age by Customer Status')\nplt.xlabel('Customer Status')\nplt.ylabel('Firm Age')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\nProportion of firms by region and customer status:\nregion         Midwest  Northeast  Northwest     South  Southwest\niscustomer                                                       \nNon-Customer  0.183513   0.267910   0.155054  0.153091   0.240432\nCustomer      0.076923   0.681913   0.060291  0.072765   0.108108\n\n\n\n\n\n\n\n\n\n\nSummary of firm age by customer status:\n               count       mean       std   min   25%   50%    75%   max\niscustomer                                                              \nNon-Customer  1019.0  26.101570  6.945426   9.0  21.0  25.5  31.25  47.5\nCustomer       481.0  26.900208  7.814678  10.0  20.5  26.5  32.50  49.0\n\n\n\n\n\n\n\n\n\n\nFrom the output, we observe notable regional differences between Blueprinty customers and non-customers. A large proportion of Blueprinty customers are concentrated in the Northeast region (approximately 68%), whereas non-customers are more evenly distributed across regions, with the Northeast representing only about 27% of them. In contrast, non-customers have substantially higher representation in regions like the Midwest, South, and Southwest, compared to customers.\nI observe that the age distributions of firms using Blueprinty and those not using it are fairly similar, but with some subtle differences. The mean firm age is slightly higher for customers (26.9 years) than for non-customers (26.1 years), but the difference is small. Overall, the age distributions for customers and non-customers are very similar.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nWe have the likelihood for_ \\(Y \\sim \\text{Poisson}(\\lambda)\\) as follows:\n\\[\nL(\\lambda \\mid Y_1, Y_2, \\dots, Y_n) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!} = \\frac{e^{-n\\lambda} \\lambda^{\\sum_{i=1}^n Y_i}}{\\prod_{i=1}^n Y_i!}\n\\]\nNext, we code the log-likelihood function for the Poisson model as a function of lambda and Y:\n\ndef poisson_loglikelihood(lambda_, Y):\n    Y = np.array(Y)\n    \n    if lambda_ &lt;= 0:\n        return -np.inf\n    \n    n = len(Y)\n    log_likelihood = -n * lambda_ + np.sum(Y * np.log(lambda_)) - np.sum(gammaln(Y + 1))\n    \n    return log_likelihood\n\nWe use the poisson_loglikelihood function that we programmed above to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas:\n\nY_observed = blueprinty_data['patents'].values\n\nlambda_values = np.linspace(0.1, 15, 300)\nlog_likelihoods = [poisson_loglikelihood(lam, Y_observed) for lam in lambda_values]\n\nplt.figure(figsize=(7, 6))\nplt.plot(lambda_values, log_likelihoods, color='blue')\nplt.title('Poisson Log-Likelihood as a Function of λ')\nplt.xlabel('λ (Poisson Rate)')\nplt.ylabel('Log-Likelihood')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWe compute:\n\nlambda_mle = blueprinty_data['patents'].mean()\nprint(f\"MLE for λ (lambda): {lambda_mle:.4f}\")\n\nMLE for λ (lambda): 3.6847\n\n\nWe take the first derivative of the log-likelihood, set it equal to zero and solve for lambda:\n\\[\n\\ell(\\lambda) = -n\\lambda + \\sum_{i=1}^n Y_i \\log(\\lambda) - \\sum_{i=1}^n \\log(Y_i!)\n\\]\nSince the last term does not depend on lambda, the derivative of the log-likelihood with respect to lambda is:\n\\[\n\\frac{d\\ell}{d\\lambda} = -n + \\sum_{i=1}^n \\frac{Y_i}{\\lambda} = -n + \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i\n\\]\nSet this derivative equal to zero:\n\\[\n-n + \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i = 0\n\\quad \\Rightarrow \\quad\n\\lambda = \\frac{1}{n} \\sum_{i=1}^n Y_i = \\bar{Y}\n\\]\nThe MLE of lambda is:\n\\[\n\\hat{\\lambda}_{\\text{MLE}} = \\bar{Y}\n\\]\nWe find the MLE by optimizing your likelihood function using optimize() in Python:\n\nY_observed = blueprinty_data['patents'].values\n\ndef neg_poisson_loglikelihood(lambda_):\n    if lambda_ &lt;= 0:\n        return np.inf\n    n = len(Y_observed)\n    return -(-n * lambda_ + np.sum(Y_observed * np.log(lambda_)) - np.sum(gammaln(Y_observed + 1)))\n\nresult = minimize_scalar(neg_poisson_loglikelihood, bounds=(0.01, 50), method='bounded')\n\nlambda_mle = result.x\nprint(f\"MLE for λ using numerical optimization: {lambda_mle:.4f}\")\n\nMLE for λ using numerical optimization: 3.6847\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nWe update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector:\n\ndef poisson_loglikelihood(beta, y, X):\n    beta = np.asarray(beta)\n    y = np.asarray(y)\n    X = np.asarray(X)\n    eta = X @ beta\n    eta = np.clip(eta, -30, 30)\n    mu = np.exp(eta)\n    log_lik = np.sum(y * np.log(mu + 1e-10) - mu - gammaln(y + 1))\n    return log_lik\n\ndef neg_poisson_loglikelihood(beta, y, X):\n    return -poisson_loglikelihood(beta, y, X)\n\nNext, we use the function we just programmed along with Python’s sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates:\n\nblueprinty_data['age_squared'] = blueprinty_data['age'].astype(float) ** 2\nregion_dummies = pd.get_dummies(blueprinty_data['region'], prefix='region', drop_first=True)\n\nfeatures = pd.concat([\n    blueprinty_data[['age', 'age_squared', 'iscustomer']].astype(float),\n    region_dummies.astype(float)\n], axis=1)\n\nX = sm.add_constant(features)\ny = blueprinty_data['patents'].astype(float)\n\n# Convert to numpy arrays\nX_array = X.values\ny_array = y.values\nbeta_init = np.zeros(X_array.shape[1])\n\nres = minimize(\n    neg_poisson_loglikelihood,\n    beta_init,\n    args=(y_array, X_array),\n    method='BFGS',\n    options={'disp': False}\n)\n\nbeta_mle = res.x\n\ndef compute_hessian(func, x, *args, epsilon=1e-5):\n    n = len(x)\n    hessian = np.zeros((n, n))\n\n    def gradient(x_val):\n        return approx_fprime(x_val, func, epsilon, *args)\n\n    base_grad = gradient(x)\n    for i in range(n):\n        x_shifted = np.array(x, dtype=float)\n        x_shifted[i] += epsilon\n        grad_shifted = gradient(x_shifted)\n        hessian[i] = (grad_shifted - base_grad) / epsilon\n\n    return 0.5 * (hessian + hessian.T)\n\nhessian = compute_hessian(neg_poisson_loglikelihood, beta_mle, y_array, X_array)\ncov_matrix = np.linalg.inv(hessian)\nby_hand_std_errs = np.sqrt(np.diag(cov_matrix))\n\ncol_names = X.columns.tolist()  # Define col_names based on X DataFrame\nby_hand_results = pd.DataFrame({\n    'MLE Coef': beta_mle,\n    'Std. Error': by_hand_std_errs\n}, index=col_names[:len(beta_mle)])\n\nprint(\"\\nBeta Parameter Estimates and Standard Errors from Hessian\")\nprint(by_hand_results)\n\n\nBeta Parameter Estimates and Standard Errors from Hessian\n                  MLE Coef  Std. Error\nconst            -0.509846    0.181531\nage               0.148695    0.013675\nage_squared      -0.002972    0.000252\niscustomer        0.207613    0.030895\nregion_Northeast  0.029155    0.043624\nregion_Northwest -0.017580    0.053778\nregion_South      0.056549    0.052660\nregion_Southwest  0.050573    0.047196\n\n\nNext, we check our results using Python sm.GLM() function:\n\nblueprinty_data['age_squared'] = blueprinty_data['age'] ** 2\n\nglm_model = smf.glm(\n    formula=\"patents ~ age + age_squared + C(region) + iscustomer\",\n    data=blueprinty_data,\n    family=sm.families.Poisson()\n)\nglm_results = glm_model.fit()\nsummary_df = pd.DataFrame({\n    'Variable': glm_results.params.index,\n    'Coefficient': glm_results.params.values,\n    'Std. Error': glm_results.bse.values,\n    'p-value': glm_results.pvalues.round(4)\n})\n\nprint(summary_df)\n\n                                      Variable  Coefficient  Std. Error  \\\nIntercept                            Intercept    -0.508920    0.183179   \nC(region)[T.Northeast]  C(region)[T.Northeast]     0.029170    0.043625   \nC(region)[T.Northwest]  C(region)[T.Northwest]    -0.017575    0.053781   \nC(region)[T.South]          C(region)[T.South]     0.056561    0.052662   \nC(region)[T.Southwest]  C(region)[T.Southwest]     0.050576    0.047198   \niscustomer[T.1]                iscustomer[T.1]     0.207591    0.030895   \nage                                        age     0.148619    0.013869   \nage_squared                        age_squared    -0.002970    0.000258   \n\n                        p-value  \nIntercept                0.0055  \nC(region)[T.Northeast]   0.5037  \nC(region)[T.Northwest]   0.7438  \nC(region)[T.South]       0.2828  \nC(region)[T.Southwest]   0.2839  \niscustomer[T.1]          0.0000  \nage                      0.0000  \nage_squared              0.0000  \n\n\nThe coefficients and standard errors for the beta parameter estimates are the same using the two different methods.\nInterpretation of the results:\nThe coefficient for iscustomer (0.2076) translates to an approximate 23.4% increase in expected patent count:\n\\[\n\\exp(0.2076) - 1 \\approx 0.234\n\\]\nThis means that firms using Blueprinty software receive ~23.4% more patents.\nThe analysis is observational. Without running an experiment to collect data, the results show association, not causation.\nThe intercept term has a coefficient of -0.5098 (standard error: 0.1815), which represents the baseline log count of patents when all predictors are zero, although it is not directly interpretable in isolation. Firm age shows a positive and statistically significant effect on patent output, with a coefficient of 0.1487 (standard error: 0.0137), indicating that older firms tend to receive more patents. However, this effect diminishes with age, as evidenced by the negative and significant coefficient on age squared (-0.0030, standard error: 0.0003), suggesting a nonlinear relationship where the benefit of age tapers off. Most notably, firms that are customers of Blueprinty software have a coefficient of 0.2076 (standard error: 0.0309), which is statistically significant and implies that, holding other factors constant, these firms receive approximately 23.4% more patents than non-customers. While regional dummy variables are included in the model, their coefficients are mixed in sign and not statistically significant, indicating that region does not have a meaningful impact on patent counts.\n\nX_0 = X.copy()\nX_1 = X.copy()\nX_0['iscustomer'] = 0\nX_1['iscustomer'] = 1\n\ny_pred_0 = np.exp(X_0.values @ beta_mle)\ny_pred_1 = np.exp(X_1.values @ beta_mle)\n\ndelta_y = y_pred_1 - y_pred_0\naverage_effect = np.mean(delta_y)\n\nprint(average_effect.round(4))\n\n0.7929\n\n\nAfter fitting a Poisson regression model to predict patent counts based on firm characteristics, we simulated two scenarios for each firm: one where the firm is a Blueprinty customer (iscustomer = 1) and one where it is not (iscustomer = 0). Holding all other characteristics constant, we predicted the number of patents in each scenario.\nThe average difference in predicted patent counts between these two scenarios was 0.79 patents per firm.\nThis means that, on average, firms using Blueprinty’s software are expected to receive 0.79 more patents over five years than similar firms that do not use the software. It reinforces the statistical finding that Blueprinty’s software use is associated with improved patent outcomes."
  },
  {
    "objectID": "blog/project2/hw2_questions.html#blueprinty-case-study",
    "href": "blog/project2/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\nWe read in the dataset for Blueprinty and view the first 5 rows of data to understand what our data contains:\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import t\nimport statsmodels.formula.api as smf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom scipy.optimize import minimize_scalar\nfrom scipy.optimize import minimize\nfrom numpy.linalg import inv\nfrom scipy.optimize import minimize, approx_fprime\nimport patsy\nimport warnings\nfrom scipy.special import gammaln\nfrom statsmodels.genmod.families import Poisson\nwarnings.filterwarnings('ignore')\n\nblueprinty_data = pd.read_csv('blueprinty.csv')\nblueprinty_data.head(5)\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\nWe build compute the mean of number of patents by customer status (i.e., customers and non-customers):\n\nblueprinty_data['iscustomer'] = blueprinty_data['iscustomer'].astype('category')\n\nmean_patents = blueprinty_data.groupby('iscustomer', observed=True)['patents'].mean()\nprint(\"Mean number of patents by customer status:\")\nprint(mean_patents.rename(index={0: 'Non-Customer', 1: 'Customer'}))\n\nMean number of patents by customer status:\niscustomer\nNon-Customer    3.473013\nCustomer        4.133056\nName: patents, dtype: float64\n\n\nThe average number of patents for non-customers is 3.473013 while the average number of patents for customers is 4.133056.\nNext, we build and compare histograms for non-customers and customers:\n\nfig, axes = plt.subplots(2, 1, figsize=(7, 8), sharex=True)\n\nsns.histplot(\n    blueprinty_data[blueprinty_data['iscustomer'] == 0],\n    x='patents',\n    binwidth=1,\n    kde=False,\n    stat='density',\n    ax=axes[0],\n    color='skyblue'\n)\naxes[0].set_title('Non-Customers')\naxes[0].set_ylabel('Density')\naxes[0].grid(True)\n\nsns.histplot(\n    blueprinty_data[blueprinty_data['iscustomer'] == 1],\n    x='patents',\n    binwidth=1,\n    kde=False,\n    stat='density',\n    ax=axes[1],\n    color='salmon'\n)\naxes[1].set_title('Customers')\naxes[1].set_xlabel('Number of Patents')\naxes[1].set_ylabel('Density')\naxes[1].grid(True)\n\nplt.suptitle('Patent Distribution: Customers vs Non-Customers', fontsize=16)\nplt.tight_layout(rect=[0, 0, 1, 0.95])\nplt.show()\n\n\n\n\n\n\n\n\nThe histograms reveal that both customers and non-customers exhibit right-skewed distributions in the number of patents, with most firms having relatively few patents and fewer firms achieving higher counts. Customers tend to cluster more heavily around the 0 to 4 patent range. Both customers and non-customers have long tails, indicating that while most firms have few patents, there are a small number of firms with a very high number of patents. The distribution for non-customers is slightly more concentrated around the lower patent counts compared to customers, suggesting that blueprinty could be effective in generating achieving patents.\nCustomers tend to have a higher number of patents than non-customers (as seen by a higher mean: 4.13 for customers and 3.47 for non-customers), with a noticeable increase in the 5 to 10 patent range, with customers having higher density in this range.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\nblueprinty_data['region'] = blueprinty_data['region'].astype('category')\n\nregion_counts = blueprinty_data.groupby('iscustomer', observed=True)['region'].value_counts(normalize=True).unstack()\nprint(\"Proportion of firms by region and customer status:\")\nprint(region_counts.rename(index={0: 'Non-Customer', 1: 'Customer'}))\nprint()\n\nregion_plot_data = blueprinty_data.copy()\nregion_plot_data['iscustomer'] = region_plot_data['iscustomer'].map({0: 'Non-Customer', 1: 'Customer'})\n\nplt.figure(figsize=(7, 6))\nsns.countplot(data=region_plot_data, x='region', hue='iscustomer')\nplt.title('Region Distribution by Customer Status')\nplt.xlabel('Region')\nplt.ylabel('Count')\nplt.legend(title='Customer Status')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\nage_summary = blueprinty_data.groupby('iscustomer', observed=True)['age'].describe()\nprint(\"Summary of firm age by customer status:\")\nprint(age_summary.rename(index={0: 'Non-Customer', 1: 'Customer'}))\nprint()\n\nplt.figure(figsize=(7, 6))\nsns.boxplot(data=blueprinty_data, x='iscustomer', y='age', palette='Set2')\nplt.xticks([0, 1], ['Non-Customer', 'Customer'])\nplt.title('Firm Age by Customer Status')\nplt.xlabel('Customer Status')\nplt.ylabel('Firm Age')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\nProportion of firms by region and customer status:\nregion         Midwest  Northeast  Northwest     South  Southwest\niscustomer                                                       \nNon-Customer  0.183513   0.267910   0.155054  0.153091   0.240432\nCustomer      0.076923   0.681913   0.060291  0.072765   0.108108\n\n\n\n\n\n\n\n\n\n\nSummary of firm age by customer status:\n               count       mean       std   min   25%   50%    75%   max\niscustomer                                                              \nNon-Customer  1019.0  26.101570  6.945426   9.0  21.0  25.5  31.25  47.5\nCustomer       481.0  26.900208  7.814678  10.0  20.5  26.5  32.50  49.0\n\n\n\n\n\n\n\n\n\n\nFrom the output, we observe notable regional differences between Blueprinty customers and non-customers. A large proportion of Blueprinty customers are concentrated in the Northeast region (approximately 68%), whereas non-customers are more evenly distributed across regions, with the Northeast representing only about 27% of them. In contrast, non-customers have substantially higher representation in regions like the Midwest, South, and Southwest, compared to customers.\nI observe that the age distributions of firms using Blueprinty and those not using it are fairly similar, but with some subtle differences. The mean firm age is slightly higher for customers (26.9 years) than for non-customers (26.1 years), but the difference is small. Overall, the age distributions for customers and non-customers are very similar.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nWe have the likelihood for_ \\(Y \\sim \\text{Poisson}(\\lambda)\\) as follows:\n\\[\nL(\\lambda \\mid Y_1, Y_2, \\dots, Y_n) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!} = \\frac{e^{-n\\lambda} \\lambda^{\\sum_{i=1}^n Y_i}}{\\prod_{i=1}^n Y_i!}\n\\]\nNext, we code the log-likelihood function for the Poisson model as a function of lambda and Y:\n\ndef poisson_loglikelihood(lambda_, Y):\n    Y = np.array(Y)\n    \n    if lambda_ &lt;= 0:\n        return -np.inf\n    \n    n = len(Y)\n    log_likelihood = -n * lambda_ + np.sum(Y * np.log(lambda_)) - np.sum(gammaln(Y + 1))\n    \n    return log_likelihood\n\nWe use the poisson_loglikelihood function that we programmed above to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas:\n\nY_observed = blueprinty_data['patents'].values\n\nlambda_values = np.linspace(0.1, 15, 300)\nlog_likelihoods = [poisson_loglikelihood(lam, Y_observed) for lam in lambda_values]\n\nplt.figure(figsize=(7, 6))\nplt.plot(lambda_values, log_likelihoods, color='blue')\nplt.title('Poisson Log-Likelihood as a Function of λ')\nplt.xlabel('λ (Poisson Rate)')\nplt.ylabel('Log-Likelihood')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWe compute:\n\nlambda_mle = blueprinty_data['patents'].mean()\nprint(f\"MLE for λ (lambda): {lambda_mle:.4f}\")\n\nMLE for λ (lambda): 3.6847\n\n\nWe take the first derivative of the log-likelihood, set it equal to zero and solve for lambda:\n\\[\n\\ell(\\lambda) = -n\\lambda + \\sum_{i=1}^n Y_i \\log(\\lambda) - \\sum_{i=1}^n \\log(Y_i!)\n\\]\nSince the last term does not depend on lambda, the derivative of the log-likelihood with respect to lambda is:\n\\[\n\\frac{d\\ell}{d\\lambda} = -n + \\sum_{i=1}^n \\frac{Y_i}{\\lambda} = -n + \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i\n\\]\nSet this derivative equal to zero:\n\\[\n-n + \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i = 0\n\\quad \\Rightarrow \\quad\n\\lambda = \\frac{1}{n} \\sum_{i=1}^n Y_i = \\bar{Y}\n\\]\nThe MLE of lambda is:\n\\[\n\\hat{\\lambda}_{\\text{MLE}} = \\bar{Y}\n\\]\nWe find the MLE by optimizing your likelihood function using optimize() in Python:\n\nY_observed = blueprinty_data['patents'].values\n\ndef neg_poisson_loglikelihood(lambda_):\n    if lambda_ &lt;= 0:\n        return np.inf\n    n = len(Y_observed)\n    return -(-n * lambda_ + np.sum(Y_observed * np.log(lambda_)) - np.sum(gammaln(Y_observed + 1)))\n\nresult = minimize_scalar(neg_poisson_loglikelihood, bounds=(0.01, 50), method='bounded')\n\nlambda_mle = result.x\nprint(f\"MLE for λ using numerical optimization: {lambda_mle:.4f}\")\n\nMLE for λ using numerical optimization: 3.6847\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nWe update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector:\n\ndef poisson_loglikelihood(beta, y, X):\n    beta = np.asarray(beta)\n    y = np.asarray(y)\n    X = np.asarray(X)\n    eta = X @ beta\n    eta = np.clip(eta, -30, 30)\n    mu = np.exp(eta)\n    log_lik = np.sum(y * np.log(mu + 1e-10) - mu - gammaln(y + 1))\n    return log_lik\n\ndef neg_poisson_loglikelihood(beta, y, X):\n    return -poisson_loglikelihood(beta, y, X)\n\nNext, we use the function we just programmed along with Python’s sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates:\n\nblueprinty_data['age_squared'] = blueprinty_data['age'].astype(float) ** 2\nregion_dummies = pd.get_dummies(blueprinty_data['region'], prefix='region', drop_first=True)\n\nfeatures = pd.concat([\n    blueprinty_data[['age', 'age_squared', 'iscustomer']].astype(float),\n    region_dummies.astype(float)\n], axis=1)\n\nX = sm.add_constant(features)\ny = blueprinty_data['patents'].astype(float)\n\n# Convert to numpy arrays\nX_array = X.values\ny_array = y.values\nbeta_init = np.zeros(X_array.shape[1])\n\nres = minimize(\n    neg_poisson_loglikelihood,\n    beta_init,\n    args=(y_array, X_array),\n    method='BFGS',\n    options={'disp': False}\n)\n\nbeta_mle = res.x\n\ndef compute_hessian(func, x, *args, epsilon=1e-5):\n    n = len(x)\n    hessian = np.zeros((n, n))\n\n    def gradient(x_val):\n        return approx_fprime(x_val, func, epsilon, *args)\n\n    base_grad = gradient(x)\n    for i in range(n):\n        x_shifted = np.array(x, dtype=float)\n        x_shifted[i] += epsilon\n        grad_shifted = gradient(x_shifted)\n        hessian[i] = (grad_shifted - base_grad) / epsilon\n\n    return 0.5 * (hessian + hessian.T)\n\nhessian = compute_hessian(neg_poisson_loglikelihood, beta_mle, y_array, X_array)\ncov_matrix = np.linalg.inv(hessian)\nby_hand_std_errs = np.sqrt(np.diag(cov_matrix))\n\ncol_names = X.columns.tolist()  # Define col_names based on X DataFrame\nby_hand_results = pd.DataFrame({\n    'MLE Coef': beta_mle,\n    'Std. Error': by_hand_std_errs\n}, index=col_names[:len(beta_mle)])\n\nprint(\"\\nBeta Parameter Estimates and Standard Errors from Hessian\")\nprint(by_hand_results)\n\n\nBeta Parameter Estimates and Standard Errors from Hessian\n                  MLE Coef  Std. Error\nconst            -0.509846    0.181531\nage               0.148695    0.013675\nage_squared      -0.002972    0.000252\niscustomer        0.207613    0.030895\nregion_Northeast  0.029155    0.043624\nregion_Northwest -0.017580    0.053778\nregion_South      0.056549    0.052660\nregion_Southwest  0.050573    0.047196\n\n\nNext, we check our results using Python sm.GLM() function:\n\nblueprinty_data['age_squared'] = blueprinty_data['age'] ** 2\n\nglm_model = smf.glm(\n    formula=\"patents ~ age + age_squared + C(region) + iscustomer\",\n    data=blueprinty_data,\n    family=sm.families.Poisson()\n)\nglm_results = glm_model.fit()\nsummary_df = pd.DataFrame({\n    'Variable': glm_results.params.index,\n    'Coefficient': glm_results.params.values,\n    'Std. Error': glm_results.bse.values,\n    'p-value': glm_results.pvalues.round(4)\n})\n\nprint(summary_df)\n\n                                      Variable  Coefficient  Std. Error  \\\nIntercept                            Intercept    -0.508920    0.183179   \nC(region)[T.Northeast]  C(region)[T.Northeast]     0.029170    0.043625   \nC(region)[T.Northwest]  C(region)[T.Northwest]    -0.017575    0.053781   \nC(region)[T.South]          C(region)[T.South]     0.056561    0.052662   \nC(region)[T.Southwest]  C(region)[T.Southwest]     0.050576    0.047198   \niscustomer[T.1]                iscustomer[T.1]     0.207591    0.030895   \nage                                        age     0.148619    0.013869   \nage_squared                        age_squared    -0.002970    0.000258   \n\n                        p-value  \nIntercept                0.0055  \nC(region)[T.Northeast]   0.5037  \nC(region)[T.Northwest]   0.7438  \nC(region)[T.South]       0.2828  \nC(region)[T.Southwest]   0.2839  \niscustomer[T.1]          0.0000  \nage                      0.0000  \nage_squared              0.0000  \n\n\nThe coefficients and standard errors for the beta parameter estimates are the same using the two different methods.\nInterpretation of the results:\nThe coefficient for iscustomer (0.2076) translates to an approximate 23.4% increase in expected patent count:\n\\[\n\\exp(0.2076) - 1 \\approx 0.234\n\\]\nThis means that firms using Blueprinty software receive ~23.4% more patents.\nThe analysis is observational. Without running an experiment to collect data, the results show association, not causation.\nThe intercept term has a coefficient of -0.5098 (standard error: 0.1815), which represents the baseline log count of patents when all predictors are zero, although it is not directly interpretable in isolation. Firm age shows a positive and statistically significant effect on patent output, with a coefficient of 0.1487 (standard error: 0.0137), indicating that older firms tend to receive more patents. However, this effect diminishes with age, as evidenced by the negative and significant coefficient on age squared (-0.0030, standard error: 0.0003), suggesting a nonlinear relationship where the benefit of age tapers off. Most notably, firms that are customers of Blueprinty software have a coefficient of 0.2076 (standard error: 0.0309), which is statistically significant and implies that, holding other factors constant, these firms receive approximately 23.4% more patents than non-customers. While regional dummy variables are included in the model, their coefficients are mixed in sign and not statistically significant, indicating that region does not have a meaningful impact on patent counts.\n\nX_0 = X.copy()\nX_1 = X.copy()\nX_0['iscustomer'] = 0\nX_1['iscustomer'] = 1\n\ny_pred_0 = np.exp(X_0.values @ beta_mle)\ny_pred_1 = np.exp(X_1.values @ beta_mle)\n\ndelta_y = y_pred_1 - y_pred_0\naverage_effect = np.mean(delta_y)\n\nprint(average_effect.round(4))\n\n0.7929\n\n\nAfter fitting a Poisson regression model to predict patent counts based on firm characteristics, we simulated two scenarios for each firm: one where the firm is a Blueprinty customer (iscustomer = 1) and one where it is not (iscustomer = 0). Holding all other characteristics constant, we predicted the number of patents in each scenario.\nThe average difference in predicted patent counts between these two scenarios was 0.79 patents per firm.\nThis means that, on average, firms using Blueprinty’s software are expected to receive 0.79 more patents over five years than similar firms that do not use the software. It reinforces the statistical finding that Blueprinty’s software use is associated with improved patent outcomes."
  },
  {
    "objectID": "blog/project2/hw2_questions.html#airbnb-case-study",
    "href": "blog/project2/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\nWe perform some EDA on the dataset. The goal is to uncover patterns, trends, and relationships across numeric and categorical features, particularly focusing on review counts, pricing, and listing characteristics. :\n\nairbnb_data = pd.read_csv('airbnb.csv')\n\nvariables = [\n    'number_of_reviews', 'price', 'room_type', 'bedrooms', 'bathrooms',\n    'review_scores_cleanliness', 'review_scores_location',\n    'review_scores_value', 'instant_bookable'\n]\n\nairbnb_freq_mean_data = airbnb_data[variables].dropna()\n\nfrequency_counts = {\n    var: airbnb_freq_mean_data[var].value_counts().sort_index()\n    for var in ['room_type', 'bedrooms', 'bathrooms', 'instant_bookable']\n}\n\nmean_values = airbnb_freq_mean_data[['number_of_reviews', 'price',\n                                     'review_scores_cleanliness',\n                                     'review_scores_location',\n                                     'review_scores_value']].mean().round(2)\n\nfreq_dfs = {k: v.reset_index().rename(columns={'index': k, k: 'count'}) for k, v in frequency_counts.items()}\n\nprint(\"Mean Values of Numeric Variables:\")\nprint(mean_values)\n\nfor var, df in freq_dfs.items():\n    print(f\"\\nFrequency Counts for {var}:\")\n    print(df)\n\nnumeric_vars = [\n    'number_of_reviews', 'price',\n    'review_scores_cleanliness', 'review_scores_location',\n    'review_scores_value'\n]\n\ncategorical_vars = ['room_type', 'bedrooms', 'bathrooms', 'instant_bookable']\n\ncorr_matrix = airbnb_data[numeric_vars].corr()\n\nplt.figure(figsize=(7, 8))\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\nplt.title('Correlation Matrix of Numeric Variables')\nplt.show()\n\nscatter_pairs = [\n    ('price', 'number_of_reviews'),\n    ('bedrooms', 'number_of_reviews'),\n    ('review_scores_cleanliness', 'number_of_reviews'),\n    ('review_scores_location', 'number_of_reviews'),\n]\n\nfig, axes = plt.subplots(len(scatter_pairs), 1, figsize=(7, 20))\nfor i, (x, y) in enumerate(scatter_pairs):\n    sns.scatterplot(data=airbnb_data, x=x, y=y, ax=axes[i], alpha=0.5, color='orange', edgecolor='w')\n    axes[i].set_title(f'{y} vs. {x}')\n    axes[i].set_xlabel(x)\n    axes[i].set_ylabel(y)\n    axes[i].grid(True)\n\nplt.tight_layout()\nplt.show()\n\nMean Values of Numeric Variables:\nnumber_of_reviews             21.17\nprice                        140.21\nreview_scores_cleanliness      9.20\nreview_scores_location         9.42\nreview_scores_value            9.33\ndtype: float64\n\nFrequency Counts for room_type:\n             count  count\n0  Entire home/apt  15543\n1     Private room  13773\n2      Shared room    844\n\nFrequency Counts for bedrooms:\n    count  count\n0     0.0   2649\n1     1.0  22303\n2     2.0   3728\n3     3.0   1099\n4     4.0    288\n5     5.0     62\n6     6.0     18\n7     7.0      6\n8     8.0      4\n9     9.0      2\n10   10.0      1\n\nFrequency Counts for bathrooms:\n    count  count\n0     0.0     89\n1     0.5     66\n2     1.0  26110\n3     1.5   1250\n4     2.0   2121\n5     2.5    219\n6     3.0    192\n7     3.5     40\n8     4.0     52\n9     4.5      3\n10    5.0      8\n11    5.5      3\n12    6.0      7\n\nFrequency Counts for instant_bookable:\n   count  count\n0      f  24243\n1      t   5917"
  },
  {
    "objectID": "blog/project2/hw2_questions.html#distribution-of-key-variables",
    "href": "blog/project2/hw2_questions.html#distribution-of-key-variables",
    "title": "Poisson Regression Examples",
    "section": "Distribution of Key Variables",
    "text": "Distribution of Key Variables\n\nNumber of Reviews\n\nThe distribution is highly right-skewed.\nMost listings have fewer than 50 reviews, with a few receiving over 400.\nSuggests that a small number of listings are very popular or long-standing.\n\n\n\nPrice\n\nAlso right-skewed, with most listings under $500.\nExtreme outliers exist (e.g., prices &gt; $5000), likely luxury properties or data entry anomalies.\n\n\n\nReview Scores (Cleanliness, Location, Value)\n\nThese scores are left-skewed, heavily concentrated near 9–10.\nIndicates consistently high guest satisfaction in most listings.\n\nThe heatmap of numeric variables reveals:\n\nBedrooms and bathrooms are moderately correlated (r = 0.41), as expected.\nReview_scores_value is positively correlated with:\n\nReview_scores_cleanliness (r = 0.62)\nReview_scores_location (r = 0.45)\n\nNumber of reviews has very weak correlations with all features (e.g., r = -0.01 with price), indicating it may be influenced by unobserved factors like visibility, listing age, or marketing.\n\n\nairbnb_clean = airbnb_data[numeric_vars + categorical_vars].dropna()\n\nplt.figure(figsize=(7, 10))\nfor i, var in enumerate(numeric_vars, 1):\n    plt.subplot(3, 2, i)\n    sns.histplot(data=airbnb_clean, x=var, bins=30, kde=True, color='skyblue')\n    plt.title(f'Distribution of {var}')\n    plt.grid(True)\n\nplt.tight_layout()\nplt.show()\n\nplt.figure(figsize=(7, 8))\nfor i, var in enumerate(categorical_vars, 1):\n    plt.subplot(2, 2, i)\n    sns.countplot(data=airbnb_clean, x=var, palette='Set2')\n    plt.title(f'Count of {var}')\n    plt.grid(True)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "blog/project2/hw2_questions.html#numeric-variables",
    "href": "blog/project2/hw2_questions.html#numeric-variables",
    "title": "Poisson Regression Examples",
    "section": "Numeric Variables",
    "text": "Numeric Variables\n\nPrice\n\nNegative relationship: lower-priced listings receive more reviews.\nSuggests that budget listings may have higher turnover or broader appeal.\n\n\n\nBedrooms\n\n1- and 2-bedroom listings dominate in review count.\nLarger properties (5+ bedrooms) are rare and have fewer reviews, likely due to niche demand.\n\n\n\nReview Scores\n\nBoth cleanliness and location scores show positive relationships with review count.\nHowever, because scores cluster near 10, the variation is minimal."
  },
  {
    "objectID": "blog/project2/hw2_questions.html#categorical-variables",
    "href": "blog/project2/hw2_questions.html#categorical-variables",
    "title": "Poisson Regression Examples",
    "section": "Categorical Variables",
    "text": "Categorical Variables\n\nRoom Type\n\nMost listings are either:\n\nEntire home/apt (~15,500) or\nPrivate room (~13,700).\n\nShared rooms (~850) are far less common, likely due to lower demand for shared accommodations.\n\n\n\nBedrooms\n\n1-bedroom units dominate (~22,300), followed by 2-bedrooms.\nListings with 5+ bedrooms are rare.\n\n\n\nBathrooms\n\nMost properties report exactly 1 bathroom.\nFractional values (e.g., 1.5) reflect more detailed entries.\n\n\n\nInstant Bookable\n\nOnly about 20% of listings allow instant booking, which could limit guest conversions.\nMay reflect host caution or preferences rather than guest demand.\n\nWe build multiple models for the number of bookings as proxied by the number of reviews:\n\nairbnb_data = pd.read_csv('airbnb.csv')\n\nairbnb_data['instant_bookable'] = airbnb_data['instant_bookable'].map({'f': 0, 't': 1})\n\nairbnb_data = airbnb_data.dropna(subset=[\n    'price', 'bedrooms', 'bathrooms',\n    'review_scores_cleanliness',\n    'review_scores_location',\n    'review_scores_value',\n    'room_type',\n    'instant_bookable',\n    'number_of_reviews'\n])\n\nformula = (\n    \"number_of_reviews ~ \"\n    \"price + bedrooms + bathrooms + \"\n    \"review_scores_cleanliness + \"\n    \"review_scores_location + \"\n    \"review_scores_value + \"\n    \"C(room_type) + instant_bookable\"\n)\n\npoisson_model = smf.glm(\n    formula=formula,\n    data=airbnb_data,\n    family=sm.families.Poisson()\n).fit()\n\npoisson_summary = pd.DataFrame({\n    'Variable': poisson_model.params.index,\n    'Rate Ratio': poisson_model.params.values,\n    'Std. Error': poisson_model.bse.values,\n    'p-value': poisson_model.pvalues.values.round(4)\n})\n\nprint(\"\\nPoisson Regression Summary:\")\nprint(poisson_summary)\n\nnb_model = smf.glm(\n    formula=formula,\n    data=airbnb_data,\n    family=sm.families.NegativeBinomial()\n).fit()\n\nnb_summary = pd.DataFrame({\n    'Variable': nb_model.params.index,\n    'Rate Ratio': nb_model.params.values,\n    'Std. Error': nb_model.bse.values,\n    'p-value': nb_model.pvalues.values.round(4)\n})\n\nprint(\"\\nNegative Binomial Regression Summary:\")\nprint(nb_summary)\n\nairbnb_data_log = airbnb_data[airbnb_data['number_of_reviews'] &gt; 0].copy()\nairbnb_data_log['log_reviews'] = np.log(airbnb_data_log['number_of_reviews'])\n\nlog_formula = (\n    \"log_reviews ~ \"\n    \"price + bedrooms + bathrooms + \"\n    \"review_scores_cleanliness + \"\n    \"review_scores_location + \"\n    \"review_scores_value + \"\n    \"C(room_type) + instant_bookable\"\n)\n\nlog_linear_model = smf.ols(\n    formula=log_formula,\n    data=airbnb_data_log\n).fit()\n\nlog_summary = pd.DataFrame({\n    'Variable': log_linear_model.params.index,\n    'Coefficient': log_linear_model.params.values,\n    'Std. Error': log_linear_model.bse.values,\n    'p-value': log_linear_model.pvalues.values.round(4)\n})\n\nprint(\"\\nLog-Linear Regression Summary:\")\nprint(log_summary)\n\nlinear_model = smf.ols(\n    formula=formula,\n    data=airbnb_data\n).fit()\n\nlinear_summary = pd.DataFrame({\n    'Variable': linear_model.params.index,\n    'Coefficient': linear_model.params.values,\n    'Std. Error': linear_model.bse.values,\n    'p-value': linear_model.pvalues.values.round(4)\n})\n\nprint(\"\\nLinear Regression (OLS) Summary:\")\nprint(linear_summary)\n\n\nPoisson Regression Summary:\n                       Variable  Rate Ratio  Std. Error  p-value\n0                     Intercept    3.572486    0.016005   0.0000\n1  C(room_type)[T.Private room]   -0.014535    0.002737   0.0000\n2   C(room_type)[T.Shared room]   -0.251896    0.008618   0.0000\n3                         price   -0.000014    0.000008   0.0838\n4                      bedrooms    0.074941    0.001988   0.0000\n5                     bathrooms   -0.123999    0.003747   0.0000\n6     review_scores_cleanliness    0.113187    0.001493   0.0000\n7        review_scores_location   -0.076795    0.001607   0.0000\n8           review_scores_value   -0.091529    0.001798   0.0000\n9              instant_bookable    0.334397    0.002889   0.0000\n\nNegative Binomial Regression Summary:\n                       Variable  Rate Ratio  Std. Error  p-value\n0                     Intercept    4.187396    0.079447   0.0000\n1  C(room_type)[T.Private room]    0.007742    0.012774   0.5445\n2   C(room_type)[T.Shared room]   -0.222696    0.036749   0.0000\n3                         price   -0.000002    0.000035   0.9627\n4                      bedrooms    0.074134    0.009574   0.0000\n5                     bathrooms   -0.115186    0.017119   0.0000\n6     review_scores_cleanliness    0.195957    0.006798   0.0000\n7        review_scores_location   -0.109347    0.007938   0.0000\n8           review_scores_value   -0.208890    0.008836   0.0000\n9              instant_bookable    0.325607    0.014875   0.0000\n\nLog-Linear Regression Summary:\n                       Variable  Coefficient  Std. Error  p-value\n0                     Intercept     2.315756    0.107760   0.0000\n1  C(room_type)[T.Private room]    -0.120097    0.017255   0.0000\n2   C(room_type)[T.Shared room]    -0.196285    0.049422   0.0001\n3                         price     0.000014    0.000047   0.7714\n4                      bedrooms     0.081864    0.012950   0.0000\n5                     bathrooms    -0.094027    0.023098   0.0000\n6     review_scores_cleanliness     0.143108    0.009124   0.0000\n7        review_scores_location    -0.081868    0.010758   0.0000\n8           review_scores_value    -0.076544    0.011957   0.0000\n9              instant_bookable     0.347087    0.020193   0.0000\n\nLinear Regression (OLS) Summary:\n                       Variable  Coefficient  Std. Error  p-value\n0                     Intercept    34.741971    2.468783   0.0000\n1  C(room_type)[T.Private room]    -0.317435    0.395310   0.4220\n2   C(room_type)[T.Shared room]    -4.882449    1.132253   0.0000\n3                         price    -0.000175    0.001073   0.8701\n4                      bedrooms     1.634692    0.296692   0.0000\n5                     bathrooms    -2.630270    0.529168   0.0000\n6     review_scores_cleanliness     2.324059    0.209038   0.0000\n7        review_scores_location    -1.730719    0.246460   0.0000\n8           review_scores_value    -2.015957    0.273938   0.0000\n9              instant_bookable     7.804577    0.462615   0.0000\n\n\nWe summarize some key insights below:\n\n\nRoom Type\n\nEntire homes/apartments attract the most reviews hence the most bookings.\nPrivate and shared rooms have significantly fewer, likely due to reduced demand or smaller groups.\n\n\n\nListing Attributes\n\nMore bedrooms predict more reviews (expected).\nMore bathrooms are associated with fewer reviews — possibly due to different target audiences.\n\n\n\nReview Scores\n\nCleanliness score is a strong positive predictor of review volume.\nLocation and value show negative associations, potentially due to score compression or multicollinearity.\n\n\n\nInstant Bookable\n\nOne of the most impactful features across all models.\nListings with instant booking enabled receive approximately 30–35% more reviews on average."
  },
  {
    "objectID": "blog/project2/hw2_questions.html#model-summaries-and-interpretations",
    "href": "blog/project2/hw2_questions.html#model-summaries-and-interpretations",
    "title": "Poisson Regression Examples",
    "section": "Model Summaries and Interpretations",
    "text": "Model Summaries and Interpretations\n\nPoisson Regression\nPoisson regression models count data assuming equal mean and variance. Coefficients are exponentiated to obtain Incidence Rate Ratios (IRRs).\n\nInstant Bookable: IRR ≈ 1.40 → 40% more reviews\nReview Score: Cleanliness: IRR ≈ 1.12 → 12% more reviews per score point\nShared Room: IRR ≈ 0.78 → 22% fewer reviews vs entire home\nBathrooms: IRR ≈ 0.88 → 12% fewer reviews per additional bathroom\n\n\n\nNegative Binomial Regression\nCoefficients are exponentiated to obtain Incidence Rate Ratios (IRRs).\n\nInstant Bookable: IRR ≈ 1.39 → 39% more reviews\nReview Score: Cleanliness: IRR ≈ 1.22 → 22% more reviews\nShared Room: IRR ≈ 0.80 → 20% fewer reviews\nPrivate Room: Not statistically significant\n\n\n\nLog-Linear Regression\nCoefficients are exponentiated to obtain Incidence Rate Ratios (IRRs).\n\nInstant Bookable: +35% reviews\nReview Score: Cleanliness: +14% reviews\nShared Room: -18% reviews\nBathrooms: -9% reviews\n\n\n\nLinear Regression\n\nInstant Bookable: +7.8 reviews\nReview Score: Cleanliness: +2.3 reviews per score unit\nShared Room: -4.9 reviews\nBathrooms: -2.6 reviews"
  },
  {
    "objectID": "blog/project2/hw2_questions.html#overall-findings",
    "href": "blog/project2/hw2_questions.html#overall-findings",
    "title": "Poisson Regression Examples",
    "section": "Overall Findings",
    "text": "Overall Findings\n\nInstant Bookable consistently leads to more reviews across all models (35–40% or ~8 additional reviews).\nCleanliness Score is one of the strongest predictors (+12–22% or ~2.3 additional reviews).\nShared Rooms generally receive fewer reviews (~18–22% less)."
  }
]