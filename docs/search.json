[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Projects",
    "section": "",
    "text": "A Replication of Karlan and List (2007)\n\n\n\n\n\n\nErin Yabsley\n\n\nApr 23, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Website – MGTA 495 MKTG",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "blog/project1/hw1_questions.html",
    "href": "blog/project1/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe paper investigates how matching grants influence charitable donations. Karlan and List (2007) randomized 50,083 donors into three groups: control, matching, and challenge. The matching group was subdivided further by match ratios (1:1, 2:1, 3:1) and match thresholds ($25k, $50k, $100k, unspecified). Letters also varied in suggested ask amount. The study tested how these variations affected the likelihood someone donates (response rate) as well as the amount they give. The experiment varied the maximum match amount offered ($25K, $50K, $100K, or unspecified) and the suggested donation amount (based on past giving).\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/project1/hw1_questions.html#introduction",
    "href": "blog/project1/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe paper investigates how matching grants influence charitable donations. Karlan and List (2007) randomized 50,083 donors into three groups: control, matching, and challenge. The matching group was subdivided further by match ratios (1:1, 2:1, 3:1) and match thresholds ($25k, $50k, $100k, unspecified). Letters also varied in suggested ask amount. The study tested how these variations affected the likelihood someone donates (response rate) as well as the amount they give. The experiment varied the maximum match amount offered ($25K, $50K, $100K, or unspecified) and the suggested donation amount (based on past giving).\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/project1/hw1_questions.html#data",
    "href": "blog/project1/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import t\nimport statsmodels.formula.api as smf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\n\ndata = pd.read_stata('karlan_list_2007.dta')\nprint(data.head(5))\n\nprint(\"Data shape:\", data.shape)\n\n   treatment  control    ratio  ratio2  ratio3      size  size25  size50  \\\n0          0        1  Control       0       0   Control       0       0   \n1          0        1  Control       0       0   Control       0       0   \n2          1        0        1       0       0  $100,000       0       0   \n3          1        0        1       0       0  Unstated       0       0   \n4          1        0        1       0       0   $50,000       0       1   \n\n   size100  sizeno  ... redcty  bluecty    pwhite    pblack  page18_39  \\\n0        0       0  ...    0.0      1.0  0.446493  0.527769   0.317591   \n1        0       0  ...    1.0      0.0       NaN       NaN        NaN   \n2        1       0  ...    0.0      1.0  0.935706  0.011948   0.276128   \n3        0       1  ...    1.0      0.0  0.888331  0.010760   0.279412   \n4        0       0  ...    0.0      1.0  0.759014  0.127421   0.442389   \n\n   ave_hh_sz  median_hhincome    powner  psch_atlstba  pop_propurban  \n0       2.10          28517.0  0.499807      0.324528            1.0  \n1        NaN              NaN       NaN           NaN            NaN  \n2       2.48          51175.0  0.721941      0.192668            1.0  \n3       2.65          79269.0  0.920431      0.412142            1.0  \n4       1.85          40908.0  0.416072      0.439965            1.0  \n\n[5 rows x 51 columns]\nData shape: (50083, 51)\n\n\nThe dataset has 50,083 observations and 51 columns. The fields included in the dataset assign individuals to either control or treatment, the specifications for the treatment group, donation history of an individual, as well as some socioeconomic & political & demographic data about the individual. The variables and their descriptions are outlined below:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\nI define a function to print results from regressions in a readable format:\n\ndef print_clean_regression_results(model, variable_names=None):\n    results = model.params.to_frame('Coefficient')\n    results['Std. Error'] = model.bse\n    results['t'] = model.tvalues\n    results['p-value'] = model.pvalues\n    results = results.round(4)\n\n    if variable_names:\n        results.index = [variable_names.get(var, var) for var in results.index]\n\n    print(results)\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\nI create a function t_test_manual that breaks down the steps to compute a t-statistic:\n\ndef t_test_manual(x_treat, x_control):\n    mean_treat = np.mean(x_treat)\n    mean_control = np.mean(x_control)\n    \n    var_treat = np.var(x_treat, ddof=1)\n    var_control = np.var(x_control, ddof=1)\n    \n    n_treat = len(x_treat)\n    n_control = len(x_control)\n\n    t_stat = (mean_treat - mean_control) / np.sqrt((var_treat / n_treat) + (var_control / n_control))\n\n    df_num = (var_treat / n_treat + var_control / n_control) ** 2\n    df_denom = ((var_treat / n_treat) ** 2 / (n_treat - 1)) + ((var_control / n_control) ** 2 / (n_control - 1))\n    df = df_num / df_denom\n\n    p_value = 2 * (1 - t.cdf(np.abs(t_stat), df))\n    return t_stat, p_value\n\nI define the variables that I want to test: the number of months since last donation (mrm2), the highest previous contribution (hpa), the number of prior donations (freq), and the number of years since initial donation (years). I test these variables at the 95% confidence level and run a linear regression on each of the variables and look at the estimated coefficient on the treatment variable:\n\nvariables = ['mrm2', 'hpa', 'freq', 'years']\n\nfor var in variables:\n    treat_vals = data[data['treatment'] == 1][var].dropna()\n    control_vals = data[data['treatment'] == 0][var].dropna()\n\n    t_stat, p_val = t_test_manual(treat_vals, control_vals)\n\n    print(f\"T-test for {var}: t-statistic = {t_stat:.4f}, p-value = {p_val:.4f}\\n\")\n    model = smf.ols(f\"{var} ~ treatment\", data=data).fit()\n    print_clean_regression_results(model, variable_names={'Intercept': 'Intercept', 'treatment': 'Treatment'})\n    print()\n\nT-test for mrm2: t-statistic = 0.1195, p-value = 0.9049\n\n           Coefficient  Std. Error         t  p-value\nIntercept      12.9981      0.0935  138.9789   0.0000\nTreatment       0.0137      0.1145    0.1195   0.9049\n\nT-test for hpa: t-statistic = 0.9704, p-value = 0.3318\n\n           Coefficient  Std. Error         t  p-value\nIntercept      58.9602      0.5510  107.0054   0.0000\nTreatment       0.6371      0.6748    0.9441   0.3451\n\nT-test for freq: t-statistic = -0.1108, p-value = 0.9117\n\n           Coefficient  Std. Error        t  p-value\nIntercept       8.0473      0.0882  91.2313   0.0000\nTreatment      -0.0120      0.1080  -0.1109   0.9117\n\nT-test for years: t-statistic = -1.0909, p-value = 0.2753\n\n           Coefficient  Std. Error         t  p-value\nIntercept       6.1359      0.0426  144.0227     0.00\nTreatment      -0.0575      0.0522   -1.1030     0.27\n\n\n\nAt the 95% confidence level, none of the baseline variables differ significantly between the treatment and control groups as all of the p-values associated with the variables are larger than 0.05. This supports random assignment - it shows that baseline characteristics are balanced between the treatment and control groups. As a result, we can conclude that any observed differences in charitable giving outcomes are due to the treatment and not pre-existing group differences."
  },
  {
    "objectID": "blog/project1/hw1_questions.html#experimental-results",
    "href": "blog/project1/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\nI create a barplot that shows the proportion of people who donated in the control group and the treatment group:\n\ndonation_rates = data.groupby('treatment')['gave'].mean()\n\nplt.figure(figsize=(8, 6))\nsns.barplot(\n    x=donation_rates.index, \n    y=donation_rates.values, \n    palette=['skyblue', 'salmon']\n)\nplt.xticks([0, 1], ['Control', 'Treatment'])\nplt.ylabel('Proportion Donated')\nplt.title('Donation Rates: Control vs Treatment')\nplt.ylim(0, 0.03)\nplt.show()\n\n/var/folders/j9/vvtdz6r56zg0sw78kg9kfsk00000gn/T/ipykernel_2715/1576349855.py:4: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(\n\n\n\n\n\n\n\n\n\nNext, I run a t-test between the treatment and control groups on the binary outcome of whether any charitable donation was made:\n\ntreat_group = data[data['treatment'] == 1]['gave']\ncontrol_group = data[data['treatment'] == 0]['gave']\n\nt_stat, p_val = t_test_manual(treat_group, control_group)\nprint(f\"T-test on gave: t = {t_stat:.4f}, p-value = {p_val:.4f}\")\n\nT-test on gave: t = 3.2095, p-value = 0.0013\n\n\nI then run a linear regression on whether any charitable donation was made with treatment as the explanatory variable:\n\nlinear_model = smf.ols('gave ~ treatment', data=data).fit()\nprint_clean_regression_results(linear_model, variable_names={'Intercept': 'Intercept', 'treatment': 'Treatment'})\n\ntreatment_summary = linear_model.summary2().tables[1].loc['treatment']\n\nprint(f\"\\nLinear Regression on 'gave ~ treatment': \\nCoefficient on treatment: {treatment_summary['Coef.']:.4f}, \\nt: {treatment_summary['t']:.4f}, p-value: {treatment_summary['P&gt;|t|']:.4f}\")\n\n           Coefficient  Std. Error        t  p-value\nIntercept       0.0179      0.0011  16.2246   0.0000\nTreatment       0.0042      0.0013   3.1014   0.0019\n\nLinear Regression on 'gave ~ treatment': \nCoefficient on treatment: 0.0042, \nt: 3.1014, p-value: 0.0019\n\n\nPeople in the treatment group were 0.42% more likely to donate than those in the control group, and this effect is statistically significant at a 95% confidence level as the p-value for the treatment variable is 0.0019 (0.0019 &lt; 0.05). This supports the idea that matching gift offers nudge more people to give, even if the absolute effect seems small.\nWhat we learn about human behavior: Social & Psychological Motivation\nEven a small match taps into social proof and reciprocity, which are powerful motivators for charitable giving. The presence of a matching gift offer can create a sense of urgency and social pressure to contribute, leading to increased donations. This aligns with the concept of social norms, where individuals are influenced by the behavior of others in their social group. The matching gift offer serves as a signal that others are also contributing, which can motivate individuals to join in and support the cause.\nDonation behavior are not purely altruistic - if it was, we might expect people to give regardless of match framing. Instead, people are motivated by cues about effectiveness and urgency, suggesting many give in part for “warm glow” satisfaction - the emotional reward of feeling generous and impactful.\nI now run a probit regression where the outcome variable is whether any charitable donation was made and the explanatory variable is assignment to treatment or control:\n\nprobit_model = smf.probit('gave ~ treatment', data=data).fit()\nprint_clean_regression_results(probit_model, variable_names={'Intercept': 'Intercept', 'treatment': 'Treatment'})\n\nmarginal_effects = probit_model.get_margeff()\nprint(marginal_effects.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n           Coefficient  Std. Error        t  p-value\nIntercept      -2.1001      0.0233 -90.0728   0.0000\nTreatment       0.0868      0.0279   3.1129   0.0019\n       Probit Marginal Effects       \n=====================================\nDep. Variable:                   gave\nMethod:                          dydx\nAt:                           overall\n==============================================================================\n                dy/dx    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\ntreatment      0.0043      0.001      3.104      0.002       0.002       0.007\n==============================================================================\n\n\nThe positive coefficient on treatment means that receiving a matching offer increases the probability of donating. The treatment effect is positive and is statistically significant at a 1% significance level: the p-value associated with the treatment variable is 0.002 as seen in the probit regression. This suggests humans are sensitive to social validation when making charitable decisions.\nThe results from the probit regression confirm the results reported in Table 3 of the paper. From our probit regression, I find that the marginal effect of the treatment is 0.0043 with a standard error of 0.001, and column 1 of Table 3 reports a marginal effect of 0.004 (rounded) with standard error 0.001\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\nI conduct a series of t-tests to test whether the size of the match ratio has an effect on whether people donate or not:\n\nratios = [1, 2, 3]\npairwise_results = {}\n\nfor i in range(len(ratios)):\n    for j in range(i + 1, len(ratios)):\n        r1, r2 = ratios[i], ratios[j]\n        group1 = data[data['ratio'] == r1]['gave']\n        group2 = data[data['ratio'] == r2]['gave']\n\n        t_stat, p_val = t_test_manual(group1, group2)\n        pairwise_results[f'{r1}:{r2}'] = {'t_stat': t_stat, 'p_value': p_val}\n\nprint(\"Pairwise t-test Results (Donation Rates by Match Ratio):\")\nfor comparison, result in pairwise_results.items():\n    print(f\"{comparison} \\t t-stat: {result['t_stat']:.4f}, p-value: {result['p_value']:.4f}\")\n\nPairwise t-test Results (Donation Rates by Match Ratio):\n1:2      t-stat: -0.9650, p-value: 0.3345\n1:3      t-stat: -1.0150, p-value: 0.3101\n2:3      t-stat: -0.0501, p-value: 0.9600\n\n\nNone of the differences across match ratios are significant at a 5% significance level. All of the p-values are larger than 0.05 (0.3345, 0.3101, 0.9600).\nThis demonstrates that higher match ratios, such as 3:1 compared to 1:1, do not lead to a statistically significantly greater increase in donor participation. A basic 1:1 match performs almost as well, making it a more cost-effective option. This indicates that nonprofits can generate comparable donor response using lower match ratios, which may help maximize the impact of their fundraising resources.\nFrom the paper, the author makes the comment: “the figures suggest that neither the match threshold nor the example amount had a meaningful influence on behavior.” The results of the t-test reinforce this comment as the different match ratios do not have a significant effect on the donation.\nI approach the same problem using a regression approach. I create the variable ratio1 then regress gave on ratio1, ratio2, and ratio3:\n\ndata['ratio1'] = (data['ratio'] == 1).astype(int)\nregression_model = smf.ols('gave ~ ratio1 + ratio2 + ratio3', data=data).fit()\nprint_clean_regression_results(regression_model, variable_names={\n    'Intercept': 'Intercept',\n    'ratio1': 'Match 1:1',\n    'ratio2': 'Match 2:1',\n    'ratio3': 'Match 3:1'\n})\n\n           Coefficient  Std. Error        t  p-value\nIntercept       0.0179      0.0011  16.2245   0.0000\nMatch 1:1       0.0029      0.0017   1.6615   0.0966\nMatch 2:1       0.0048      0.0017   2.7445   0.0061\nMatch 3:1       0.0049      0.0017   2.8016   0.0051\n\n\nI interpret the coefficients and their statistical precision: the intercept (1.79%) is the baseline donation rate for the control group. The ratio1 (1:1) adds +0.29% to that (total ≈ 2.68%), but the result is significant at a 90% confidence level (p-value ≈ 0.096). ratio2 (2:1) and ratio3 (3:1) both raise donation rates by 0.48 and 0.49% respectively to 2.66 and 2.68% and are statistically significant at the 1% level (with p-values of 0.0061 and 0.0051 respectively).\nI calculate the response rate difference between the 1:1 and 2:1 match ratios and the 2:1 and 3:1 ratios. I do this directly from the data:\n\nresponse_rates = data.groupby('ratio', observed=False)['gave'].mean()\n\ncoef_ratio2 = regression_model.params['ratio2']\ncoef_ratio3 = regression_model.params['ratio3']\nintercept = regression_model.params['Intercept']\n\ndiff_data_2_1 = response_rates[2] - response_rates[1]\ndiff_data_3_2 = response_rates[3] - response_rates[2]\n\nprint(\"Response Rates from Raw Data:\")\nprint(f\"1:1 match = {response_rates[1]:.4f}\")\nprint(f\"2:1 match = {response_rates[2]:.4f}\")\nprint(f\"3:1 match = {response_rates[3]:.4f}\")\nprint(f\"Difference (2:1 - 1:1): {diff_data_2_1:.4f}\")\nprint(f\"Difference (3:1 - 2:1): {diff_data_3_2:.4f}\")\n\nResponse Rates from Raw Data:\n1:1 match = 0.0207\n2:1 match = 0.0226\n3:1 match = 0.0227\nDifference (2:1 - 1:1): 0.0019\nDifference (3:1 - 2:1): 0.0001\n\n\nThe difference from the raw data between the 1:1 match and the 2:1 match is 0.0019 and the difference from the raw data between the 1:1 match and the 3:1 match is 0.0001.\nI now calculate the response rate difference between the 1:1 and 2:1 match ratios and the 2:1 and 3:1 ratios with the fitted coefficients from the regression:\n\npred_1to1 = intercept\npred_2to1 = intercept + coef_ratio2\npred_3to1 = intercept + coef_ratio3\n\ndiff_model_2_1 = pred_2to1 - pred_1to1\ndiff_model_3_2 = pred_3to1 - pred_2to1\n\nprint(\"\\nPredicted Donation Rates from Regression Model:\")\nprint(f\"1:1 match = {pred_1to1:.4f}\")\nprint(f\"2:1 match = {pred_2to1:.4f}\")\nprint(f\"3:1 match = {pred_3to1:.4f}\")\nprint(f\"Difference (2:1 - 1:1): {diff_model_2_1:.4f}\")\nprint(f\"Difference (3:1 - 2:1): {diff_model_3_2:.4f}\")\n\n\nPredicted Donation Rates from Regression Model:\n1:1 match = 0.0179\n2:1 match = 0.0226\n3:1 match = 0.0227\nDifference (2:1 - 1:1): 0.0048\nDifference (3:1 - 2:1): 0.0001\n\n\nThe difference from the regression model between the 1:1 match and the 2:1 match is 0.0048 and the difference from the regression model between the 1:1 match and the 3:1 match is 0.0001.\nThere is no strong evidence that offering larger match ratios (e.g., 3:1 instead of 1:1) is more effective in increasing the number of donors. A simple 1:1 match is nearly just as effective, making it a more cost-efficient fundraising strategy. This suggests that organizations can achieve similar results with lower match ratios, which can be beneficial for their fundraising efforts.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\nI run a bivariate linear regression of the donation amount on the treatment status:\n\nreg_all = smf.ols(\"amount ~ treatment\", data=data).fit()\nprint(\"Regression on all participants (includes non-donors):\")\nprint_clean_regression_results(reg_all, variable_names={'Intercept': 'Intercept', 'treatment': 'Treatment'})\n\nRegression on all participants (includes non-donors):\n           Coefficient  Std. Error        t  p-value\nIntercept       0.8133      0.0674  12.0630   0.0000\nTreatment       0.1536      0.0826   1.8605   0.0628\n\n\nPeople in the control group gave $0.81 on average (including non-donors). Being in the treatment group increases the average donation by $0.15, a 19% increase, but this is only significant at a 10% significance level (p ≈ 0.06). Most people didn’t donate, making the average low because the dataset includes a lot of zeros.\nNow I limit the data to just people who made a donation and repeat the previous analysis. This regression allows me to analyze how much respondents donate conditional on donating some positive amount:\n\ndata_donated = data[data['gave'] == 1]\nreg_conditional = smf.ols(\"amount ~ treatment\", data=data_donated).fit()\nprint(\"\\nRegression on donors only (conditional on giving):\")\nprint_clean_regression_results(reg_conditional, variable_names={'Intercept': 'Intercept', 'treatment': 'Treatment'})\n\n\nRegression on donors only (conditional on giving):\n           Coefficient  Std. Error        t  p-value\nIntercept      45.5403      2.4234  18.7921   0.0000\nTreatment      -1.6684      2.8724  -0.5808   0.5615\n\n\nThe average donation among donors that gave is $45.54 in the control group. This is the intercept of the regression and is statistically significant at a 1% significance level given its p-value is ~0.\nDonors who received the matching grant treatment gave $1.67 less than control donors. This difference is not statistically significant (p = 0.561).\nThis regression confirms that among people who already decided to donate, the amount they gave did not increase with a match incentive. This confirms the statement in the paper that reads “The match increases participation, but does not significantly affect the donation amount conditional on giving.” The treatment coefficient has a causal interpretation about the effect on donors since this regression is conditional on giving. Donors were randomly assigned to treatment (match) or control (no match) and random assignment ensures that treatment is independent of unobserved confounders. So the coefficient on treatment estimates the causal effect of being offered a match.\nI make two plot: one for the treatment group and one for the control. Each plot shows a histogram of the donation amounts only among people who donated:\n\nfig, axs = plt.subplots(1, 2, figsize=(8, 5), sharey=True)\n\ncontrol_donated = data_donated[data_donated['treatment'] == 0]['amount']\nmean_control = control_donated.mean()\nsns.histplot(control_donated, bins=30, ax=axs[0], color=\"skyblue\", kde=False)\naxs[0].axvline(mean_control, color='red', linestyle='--', label=f\"Mean = {mean_control:.2f}\")\naxs[0].set_title(\"Control Group (Donors Only)\")\naxs[0].set_xlabel(\"Donation Amount\")\naxs[0].legend()\n\ntreatment_donated = data_donated[data_donated['treatment'] == 1]['amount']\nmean_treatment = treatment_donated.mean()\nsns.histplot(treatment_donated, bins=30, ax=axs[1], color=\"salmon\", kde=False)\naxs[1].axvline(mean_treatment, color='red', linestyle='--', label=f\"Mean = {mean_treatment:.2f}\")\naxs[1].set_title(\"Treatment Group (Donors Only)\")\naxs[1].set_xlabel(\"Donation Amount\")\naxs[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nI observe a similar distribution of donation amounts in both control and treatment groups. The average donation is slightly higher in control, which aligns with the regression result. Most donations cluster below $100, with the majority of the data falling between $0 to $50, and we observe a long tail.\nThe treatment effect is effective in motivating more people to give, not by getting people to give more."
  },
  {
    "objectID": "blog/project1/hw1_questions.html#simulation-experiment",
    "href": "blog/project1/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\nFirst, we compute the actual difference in the data:\n\np_control_data = data[data['control'] == 1]['gave'].mean()\np_treatment_data = data[data['treatment'] == 1]['gave'].mean()\ndiff_data = p_treatment_data - p_control_data\n\nprint(f\"Difference: {diff_data:.4f}\")\n\nDifference: 0.0042\n\n\nFrom the observed data, I find that the observed difference between the treatment group and the control group is 0.0040.\n\nLaw of Large Numbers\nI now simulate 10,000 draws from the control distribution and 10,000 draws from the treatment distribution. I then calculate a vector of 10,000 differences. I present a histogram for the sampling distributions for the treatment group, the control group, and the difference between the two groups:\n\nnp.random.seed(42)\n\np_control = 0.018\np_treatment = 0.022\nn_simulations = 10000\nsample_size = 500\n\ncontrol_means = []\ntreatment_means = []\ndiffs = []\n\nfor i in range(n_simulations):\n    control_sample = np.random.binomial(1, p_control, sample_size)\n    treatment_sample = np.random.binomial(1, p_treatment, sample_size)\n\n    control_mean = control_sample.mean()\n    treatment_mean = treatment_sample.mean()\n    diff = treatment_mean - control_mean\n\n    control_means.append(control_mean)\n    treatment_means.append(treatment_mean)\n    diffs.append(diff)\n\ncontrol_means = np.array(control_means)\ntreatment_means = np.array(treatment_means)\ndiffs = np.array(diffs)\n\nfig, axes = plt.subplots(3, 1, figsize=(8, 12), sharex=True)\n\nsns.histplot(control_means, bins=50, kde=True, color='skyblue', ax=axes[0])\naxes[0].set_title('Sampling Distribution of Control Group Means')\naxes[0].set_ylabel('Frequency')\n\nsns.histplot(treatment_means, bins=50, kde=True, color='salmon', ax=axes[1])\naxes[1].set_title('Sampling Distribution of Treatment Group Means')\naxes[1].set_ylabel('Frequency')\n\nsns.histplot(diffs, bins=50, kde=True, color='slateblue', ax=axes[2])\naxes[2].set_title('Sampling Distribution of Differences (Treatment - Control)')\naxes[2].set_xlabel('Proportion Donated')\naxes[2].set_ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nI plot the cumulative average of the vector of differences:\n\nnp.random.seed(42)\n\nn_control = 10000\nn_treatment = 10000\np_control = 0.018\np_treatment = 0.022\n\ncontrol_draws = np.random.binomial(1, p_control, n_control)\ntreatment_draws = np.random.binomial(1, p_treatment, n_treatment)\n\ndifferences = treatment_draws - control_draws\n\ncumulative_avg = np.cumsum(differences) / np.arange(1, len(differences) + 1)\n\nplt.figure(figsize=(8, 5))\nplt.plot(cumulative_avg, label='Cumulative Average of Differences', color='slateblue')\nplt.axhline(y=p_treatment - p_control, color='red', linestyle='--', label=f\"True Difference = {p_treatment - p_control:.4f}\")\nplt.title(\"Cumulative Average of Differences in Donation Rates\")\nplt.xlabel(\"Number of Samples\")\nplt.ylabel(\"Cumulative Average (Treatment - Control)\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWhat the Chart Shows: X-axis: Number of samples (from 1 up to 10,000) Y-axis: Cumulative average difference in donation rates (Treatment - Control) Blue Line: The running (cumulative) average of the difference in donation rates Red Dashed Line: The “true” or overall difference in donation rates across all data = 0.0040\nThe Law of Large Numbers says that as you increase your sample size, the sample average will converge to the true population mean.\nWhen the sample size is small (i.e., the first few hundred donors), the cumulative average fluctuates wildly, up to nearly 0.04, due to the small sample size. As the sample grows, the cumulative average smooths out and converges around 0.004, the red line. This is Law of Large Numbers in action: with more data, the sample estimate gets closer to the true treatment effect.\n\n\nCentral Limit Theorem\nIn this section of the analysis, I generated sampling distributions of the difference in donation rates between treatment and control groups by repeatedly drawing samples of different sizes (50, 200, 500, and 1000) and calculating the average difference in donation rates for each draw. I plot the histograms of the distributions:\n\nnp.random.seed(42)\n\nsample_sizes = [50, 200, 500, 1000]\nn_iterations = 1000\n\nfig, axs = plt.subplots(2, 2, figsize=(8, 10))\naxs = axs.flatten()\n\nfor i, n in enumerate(sample_sizes):\n    diffs = []\n    for _ in range(n_iterations):\n        control_sample = np.random.binomial(1, p_control, n)\n        treatment_sample = np.random.binomial(1, p_treatment, n)\n        diff = treatment_sample.mean() - control_sample.mean()\n        diffs.append(diff)\n\n    sns.histplot(diffs, bins=30, ax=axs[i], color=\"skyblue\", kde=False)\n    axs[i].axvline(0, color='black', linestyle='--', label='Zero (No Effect)')\n    axs[i].axvline(np.mean(diffs), color='red', linestyle='--', label=f'Mean = {np.mean(diffs):.4f}')\n    axs[i].set_title(f\"Sample Size = {n}\")\n    axs[i].set_xlabel(\"Difference in Sample Means\")\n    axs[i].set_ylabel(\"Frequency\")\n    axs[i].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe 4 histograms above display the distributions of sample means for different sample sizes.\nAt the smallest sample size (n = 50), the distribution of differences is wide and relatively noisy, with zero appearing near the center of the distribution. This means that with small samples, even when there is a true difference between treatment and control groups, the observed difference can often appear close to zero purely by chance: the signal is weak and buried in variability.\nAs the sample size increases to 200, the distribution becomes more concentrated, and I begin to see that zero is slightly less central. When I reach sample sizes of 500 and especially 1000, the distributions are sharply peaked and much narrower. Importantly, in these larger samples, zero is clearly in the tail of the distribution, not the center. This means that the observed differences consistently reflect the true underlying effect (here, a 0.004 increase in donation probability due to treatment), and it becomes increasingly unlikely that such a difference would be due to random chance alone.\nThe Central Limit Theorem and the Law of Large Numbers are illustrated: as sample size increases, sampling distributions of means become approximately normal and converge around the true mean. It also reinforces the intuition behind statistical significance—larger samples make it easier to detect small but real effects because zero is no longer a plausible value under the observed distribution of outcomes."
  }
]